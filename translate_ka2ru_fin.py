import os
import re
import json
import time
import zipfile
import threading
import tkinter as tk
from tkinter import filedialog, messagebox
from tkinter import ttk
from typing import Dict, List, Set, Iterable, Callable, Optional, Any
from openai import RateLimitError

import xml.etree.ElementTree as ET

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM
import torch

# ============ –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç –ø–æ —Ç–æ–∫–µ–Ω–∞–º, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—à–∞—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è ChatGPT =============

def estimate_tokens(text: str) -> int:
    """
    –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤.

    –†–∞–Ω—å—à–µ –º—ã –±—Ä–∞–ª–∏ len(text)//3 –∏ –°–ò–õ–¨–ù–û –Ω–µ–¥–æ–æ—Ü–µ–Ω–∏–≤–∞–ª–∏.
    –¢–µ–ø–µ—Ä—å —Å—á–∏—Ç–∞–µ–º –ø—Ä–∏–º–µ—Ä–Ω–æ –∫–∞–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤, —Å –Ω–µ–±–æ–ª—å—à–∏–º –∑–∞–ø–∞—Å–æ–º.
    –≠—Ç–æ –∑–∞–≤–µ–¥–æ–º–æ –∑–∞–≤—ã—à–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã, –Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è –ª–∏–º–∏—Ç–∞.
    """
    if not text:
        return 1
    # +20% –∑–∞–ø–∞—Å —Å–≤–µ—Ä—Ö—É
    return int(len(text) * 1.2)


def split_fragments_by_tokens(
    fragments: List[Dict[str, Any]],
    max_tokens_per_batch: int = 8000,
) -> Iterable[List[Dict[str, Any]]]:
    """
    –î–µ–ª–∏—Ç —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –Ω–∞ –±–∞—Ç—á–∏ —Ç–∞–∫, —á—Ç–æ–±—ã —Å—É–º–º–∞—Ä–Ω–æ–µ
    –æ—Ü–µ–Ω–æ—á–Ω–æ–µ –∫–æ–ª-–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–ª–æ max_tokens_per_batch.
    """
    batch: List[Dict[str, Any]] = []
    current_tokens = 0

    for frag in fragments:
        text = frag["text"]
        t = estimate_tokens(text)

        # –ï—Å–ª–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Å–∞–º –±–æ–ª—å—à–µ –ª–∏–º–∏—Ç–∞, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –µ–≥–æ –æ—Ç–¥–µ–ª—å–Ω–æ
        if t > max_tokens_per_batch:
            if batch:
                yield batch
                batch = []
                current_tokens = 0
            yield [frag]
            continue

        # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –ø–µ—Ä–µ–ø–æ–ª–Ω–∏—Ç –ø–∞—Ä—Ç–∏—é ‚Äì –æ—Ç–¥–∞–µ–º —Ç–µ–∫—É—â—É—é
        if batch and current_tokens + t > max_tokens_per_batch:
            yield batch
            batch = []
            current_tokens = 0

        batch.append(frag)
        current_tokens += t

    if batch:
        yield batch

def split_texts_by_tokens(
    texts: List[str],
    max_tokens_per_batch: int = 8000,
) -> Iterable[List[str]]:
    """
    –î–µ–ª–∏—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –Ω–∞ –±–∞—Ç—á–∏ —Ç–∞–∫, —á—Ç–æ–±—ã —Å—É–º–º–∞—Ä–Ω–æ–µ
    –æ—Ü–µ–Ω–æ—á–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –±–∞—Ç—á–µ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–ª–æ max_tokens_per_batch.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç estimate_tokens(), –∫–∞–∫ –∏ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞.
    """
    batch: List[str] = []
    current_tokens = 0

    for text in texts:
        if not isinstance(text, str):
            continue
        t = estimate_tokens(text)

        # –µ—Å–ª–∏ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç —Å–∞–º –ø–æ —Å–µ–±–µ –±–æ–ª—å—à–µ –ª–∏–º–∏—Ç–∞ ‚Äî –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –µ–≥–æ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –±–∞—Ç—á–µ
        if t > max_tokens_per_batch:
            if batch:
                yield batch
                batch = []
                current_tokens = 0
            yield [text]
            continue

        # –µ—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–ø–æ–ª–Ω–∏—Ç –±–∞—Ç—á ‚Äî –æ—Ç–¥–∞–µ–º —Ç–µ–∫—É—â–∏–π –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π
        if batch and current_tokens + t > max_tokens_per_batch:
            yield batch
            batch = []
            current_tokens = 0

        batch.append(text)
        current_tokens += t

    if batch:
        yield batch


# ============ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ============

DEFAULT_CHATGPT_MODEL = "gpt-4.1-mini"
CHATGPT_MODELS = [
    "gpt-4.1-mini",
    "gpt-4.1",
    "gpt-4o-mini",
    "gpt-4o",
    "gpt-5.1",
    "gpt-5-mini",
]

# –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞: –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π -> —Ü–µ–ª–µ–≤–æ–π —è–∑—ã–∫
DIRECTION_CONFIG: Dict[str, Dict[str, str]] = {
    "ka-ru": {
        "label": "–ì—Ä—É–∑–∏–Ω—Å–∫–∏–π ‚Üí –†—É—Å—Å–∫–∏–π",
        "target_language": "Russian",
        "suffix": "_ru",
    },
    "ka-en": {
        "label": "–ì—Ä—É–∑–∏–Ω—Å–∫–∏–π ‚Üí –ê–Ω–≥–ª–∏–π—Å–∫–∏–π",
        "target_language": "English",
        "suffix": "_en",
    },
}


# –¥–∏–∞–ø–∞–∑–æ–Ω—ã Unicode –¥–ª—è –≥—Ä—É–∑–∏–Ω—Å–∫–æ–≥–æ
GEORGIAN_RE = re.compile(r"[\u10A0-\u10FF\u1C90-\u1CBF]+")


# ============ –£—Ç–∏–ª–∏—Ç—ã ============

def is_docx(path: str) -> bool:
    return path.lower().endswith(".docx")


def is_xlsx(path: str) -> bool:
    return path.lower().endswith(".xlsx")


def chunks(lst: List[str], n: int) -> Iterable[List[str]]:
    for i in range(0, len(lst), n):
        yield lst[i:i + n]


def load_api_key_from_env_file(env_path: str) -> str:
    """
    –ß–∏—Ç–∞–µ—Ç .env-—Ñ–∞–π–ª –∏ –∏—â–µ—Ç OPENAI_API_KEY / API_KEY.
    –ï—Å–ª–∏ –Ω–µ –Ω–∞—à—ë–ª ‚Äî –±–µ—Ä—ë—Ç –ø–µ—Ä–≤—É—é –Ω–µ–ø—É—Å—Ç—É—é, –Ω–µ–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É –∫–∞–∫ –∫–ª—é—á.
    """
    if not os.path.exists(env_path):
        raise FileNotFoundError(f".env —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {env_path}")

    candidate = None
    with open(env_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            if "=" in line:
                k, v = line.split("=", 1)
                k = k.strip()
                v = v.strip()
                if k in ("OPENAI_API_KEY", "API_KEY"):
                    return v
                if candidate is None and v:
                    candidate = v
            else:
                if candidate is None:
                    candidate = line

    if not candidate:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–ª—é—á API –∏–∑ .env —Ñ–∞–π–ª–∞.")
    return candidate


def get_direction_code_from_label(label: str) -> str:
    for code, meta in DIRECTION_CONFIG.items():
        if meta["label"] == label:
            return code
    raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞: {label}")


# ============ –†–∞–±–æ—Ç–∞ —Å XML –≤–Ω—É—Ç—Ä–∏ DOCX/XLSX ============

def collect_docx_items(path: str) -> List[Dict[str, object]]:
    """
    DOCX ‚Äî —Ä–∞–±–æ—Ç–∞–µ–º –ø–æ –ê–ë–ó–ê–¶–ê–ú (<w:p>), –Ω–æ —Ç–µ–ø–µ—Ä—å —Å–æ–±–∏—Ä–∞–µ–º –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç—ã,
    –∞ –ü–û–õ–ù–´–ô —Å–ø–∏—Å–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ ID.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π:
      {
        "id": "word/document.xml::p17",
        "xml_name": "word/document.xml",
        "p_index": 17,
        "full_text": "<–≤–µ—Å—å —Ç–µ–∫—Å—Ç –∞–±–∑–∞—Ü–∞ –∫–∞–∫ –µ—Å—Ç—å>",
        "clean_text": "<full_text.strip()>"
      }

    –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –∞–±–∑–∞—Ü—ã, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç.
    """
    items: List[Dict[str, object]] = []

    with zipfile.ZipFile(path, "r") as zin:
        for info in zin.infolist():
            fname = info.filename
            if not (fname.startswith("word/") and fname.lower().endswith(".xml")):
                continue

            xml_bytes = zin.read(fname)
            try:
                root = ET.fromstring(xml_bytes)
            except Exception:
                continue

            m = re.match(r"\{(.*)\}", root.tag)
            ns = m.group(1) if m else ""
            p_tag = f"{{{ns}}}p"
            t_tag = f"{{{ns}}}t"

            for p_index, p in enumerate(root.iter(p_tag)):
                t_elems = list(p.iter(t_tag))
                if not t_elems:
                    continue

                parts = [(t.text or "") for t in t_elems]
                full_text = "".join(parts)
                if not full_text:
                    continue

                if not GEORGIAN_RE.search(full_text):
                    continue

                clean_text = full_text.strip()
                if not clean_text:
                    continue

                item_id = f"{fname}::p{p_index}"
                items.append({
                    "id": item_id,
                    "xml_name": fname,
                    "p_index": p_index,
                    "full_text": full_text,
                    "clean_text": clean_text,
                })

    print(f"üìÑ DOCX: –Ω–∞–π–¥–µ–Ω–æ {len(items)} –∞–±–∑–∞—Ü–µ–≤ —Å –≥—Ä—É–∑–∏–Ω—Å–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º.")
    return items


def collect_georgian_fragments_from_xml_bytes(xml_bytes: bytes) -> Set[str]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –Ω–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —É–∑–ª—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π,
    –∏ –±–µ—Ä—ë–º –∏—Ö –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ü–µ–ª–∏–∫–æ–º (strip()).
    """
    result: Set[str] = set()
    try:
        root = ET.fromstring(xml_bytes)
    except Exception:
        return result

    for elem in root.iter():
        text = elem.text
        if not text:
            continue
        if GEORGIAN_RE.search(text):
            cleaned = text.strip()
            if cleaned:
                result.add(cleaned)
    return result


def collect_fragments_xlsx(path: str) -> Set[str]:
    """
    XLSX ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º sharedStrings + worksheets.
    workbook.xml (–∏–º–µ–Ω–∞ –ª–∏—Å—Ç–æ–≤) –Ω–µ —Ç—Ä–æ–≥–∞–µ–º.
    """
    to_translate: Set[str] = set()

    with zipfile.ZipFile(path, "r") as zin:
        for info in zin.infolist():
            fname = info.filename.lower()

            if fname.startswith("xl/sharedstrings") and fname.endswith(".xml"):
                xml_bytes = zin.read(info.filename)
            elif fname.startswith("xl/worksheets/") and fname.endswith(".xml"):
                xml_bytes = zin.read(info.filename)
            else:
                continue

            frags = collect_georgian_fragments_from_xml_bytes(xml_bytes)
            to_translate.update(frags)

    print(f"üìä XLSX: –Ω–∞–π–¥–µ–Ω–æ {len(to_translate)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥—Ä—É–∑–∏–Ω—Å–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.")
    return to_translate


def replace_georgian_in_xml_bytes(xml_bytes: bytes, mapping: Dict[str, str]) -> bytes:
    """
    –î–ª—è XLSX: –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç —É–∑–ª–∞ (strip) –µ—Å—Ç—å –≤ mapping ‚Äî –∑–∞–º–µ–Ω—è–µ–º —Ü–µ–ª–∏–∫–æ–º,
    —Å–æ—Ö—Ä–∞–Ω—è—è –≤–µ–¥—É—â–∏–µ/—Ö–≤–æ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–±–µ–ª—ã.
    """
    try:
        root = ET.fromstring(xml_bytes)
    except Exception:
        return xml_bytes

    for elem in root.iter():
        text = elem.text
        if not text:
            continue

        stripped = text.strip()
        if stripped in mapping:
            prefix_len = len(text) - len(text.lstrip())
            suffix_len = len(text) - len(text.rstrip())
            prefix = text[:prefix_len]
            suffix = text[len(text) - suffix_len:] if suffix_len > 0 else ""
            new_text = mapping[stripped]
            elem.text = f"{prefix}{new_text}{suffix}"

    return ET.tostring(root, encoding="utf-8", xml_declaration=True)


def process_docx_xml_paragraphs(
    xml_bytes: bytes,
    xml_name: str,
    id_mapping: Dict[str, str],
) -> bytes:
    """
    –ü—Ä–æ–±–µ–≥–∞–µ–º –ø–æ <w:p> –≤ –æ–¥–Ω–æ–º XML-—Ñ–∞–π–ª–µ DOCX –∏, –µ—Å–ª–∏ –¥–ª—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –µ—Å—Ç—å
    –ø–µ—Ä–µ–≤–æ–¥ –≤ id_mapping, –ø–æ–¥–º–µ–Ω—è–µ–º –µ–≥–æ —Ç–µ–∫—Å—Ç. –ü—Ä–∏ —ç—Ç–æ–º:
      - –µ—Å–ª–∏ –≤ —Ç–µ–∫—É—â–µ–º –∞–±–∑–∞—Ü–µ —É–∂–µ –ù–ï–¢ –≥—Ä—É–∑–∏–Ω—Å–∫–∏—Ö –±—É–∫–≤, –º—ã –µ–≥–æ –ù–ï —Ç—Ä–æ–≥–∞–µ–º,
        –¥–∞–∂–µ –µ—Å–ª–∏ –µ–≥–æ ID –µ—Å—Ç—å –≤ id_mapping (–∑–∞—â–∏—Ç–∞ –æ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞
        –ø–æ —É–∂–µ –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É / –≤—Ä—É—á–Ω—É—é –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã–º –∫—É—Å–∫–∞–º).
    """
    try:
        root = ET.fromstring(xml_bytes)
    except Exception:
        return xml_bytes

    m = re.match(r"\{(.*)\}", root.tag)
    ns = m.group(1) if m else ""
    p_tag = f"{{{ns}}}p"
    t_tag = f"{{{ns}}}t"

    for p_index, p in enumerate(root.iter(p_tag)):
        para_id = f"{xml_name}::p{p_index}"
        if para_id not in id_mapping:
            continue

        t_elems = list(p.iter(t_tag))
        if not t_elems:
            continue

        orig_parts = [(t.text or "") for t in t_elems]
        orig_full = "".join(orig_parts)
        if not orig_full:
            continue

        # –µ—Å–ª–∏ –≤ –∞–±–∑–∞—Ü–µ —É–∂–µ –Ω–µ—Ç –≥—Ä—É–∑–∏–Ω—Å–∫–∏—Ö –±—É–∫–≤ ‚Äî –Ω–µ —Ç—Ä–æ–≥–∞–µ–º
        if not GEORGIAN_RE.search(orig_full):
            continue

        translated_clean = id_mapping[para_id]

        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–¥—É—â–∏–µ/—Ö–≤–æ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–±–µ–ª—ã –∞–±–∑–∞—Ü–∞
        lead = len(orig_full) - len(orig_full.lstrip())
        trail = len(orig_full) - len(orig_full.rstrip())
        prefix = orig_full[:lead]
        suffix = orig_full[len(orig_full) - trail:] if trail > 0 else ""
        translated_full = prefix + translated_clean + suffix

        # –í–µ—Å—å —Ç–µ–∫—Å—Ç –∞–±–∑–∞—Ü–∞ –∫–ª–∞–¥—ë–º –≤ –ø–µ—Ä–≤—ã–π <w:t>, –æ—Å—Ç–∞–ª—å–Ω—ã–µ —á–∏—Å—Ç–∏–º.
        t_elems[0].text = translated_full
        for t in t_elems[1:]:
            t.text = ""

    return ET.tostring(root, encoding="utf-8", xml_declaration=True)


def debug_scan_docx_for_georgian(path: str, max_examples: int = 20) -> None:
    """
    –û—Ç–ª–∞–¥–æ—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: —Å–∫–∞–Ω–∏—Ä—É–µ—Ç DOCX –∏ –∏—â–µ—Ç –≤—Å–µ –∞–±–∑–∞—Ü—ã, –≥–¥–µ –æ—Å—Ç–∞–ª—Å—è –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç.
    –ü–µ—á–∞—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (–∫—É—Å–æ—á–µ–∫ —Ç–µ–∫—Å—Ç–∞ + –∏–º—è XML-—Ñ–∞–π–ª–∞ + –∏–Ω–¥–µ–∫—Å –∞–±–∑–∞—Ü–∞).
    """
    count = 0
    examples = []

    with zipfile.ZipFile(path, "r") as zin:
        for info in zin.infolist():
            fname = info.filename
            if not (fname.startswith("word/") and fname.lower().endswith(".xml")):
                continue

            xml_bytes = zin.read(fname)
            try:
                root = ET.fromstring(xml_bytes)
            except Exception:
                continue

            m = re.match(r"\{(.*)\}", root.tag)
            ns = m.group(1) if m else ""
            p_tag = f"{{{ns}}}p"
            t_tag = f"{{{ns}}}t"

            for p_index, p in enumerate(root.iter(p_tag)):
                parts = []
                for t in p.iter(t_tag):
                    parts.append(t.text or "")
                full_text = "".join(parts)
                if not full_text:
                    continue
                if GEORGIAN_RE.search(full_text):
                    count += 1
                    if len(examples) < max_examples:
                        snippet = full_text.strip()
                        if len(snippet) > 120:
                            snippet = snippet[:117] + "..."
                        examples.append((fname, p_index, snippet))

    print(f"üîç –í —Ñ–∞–π–ª–µ {os.path.basename(path)} –æ—Å—Ç–∞–ª–æ—Å—å –∞–±–∑–∞—Ü–µ–≤ —Å –≥—Ä—É–∑–∏–Ω—Å–∫–∏–º: {count}")
    for fname, p_index, snippet in examples:
        print(f"  - {fname}::p{p_index}: {snippet}")


def apply_translations_docx(
    input_path: str,
    output_path: str,
    id_mapping: Dict[str, str],
    text_mapping: Optional[Dict[str, str]] = None,
    progress_callback: Optional[Callable[[float, str], None]] = None,
    start: float = 90.0,
    end: float = 100.0,
) -> None:
    with zipfile.ZipFile(input_path, "r") as zin, \
         zipfile.ZipFile(output_path, "w", compression=zipfile.ZIP_DEFLATED) as zout:

        infos = zin.infolist()
        total = len(infos)
        changed = 0

        for idx, info in enumerate(infos, start=1):
            fname = info.filename
            data = zin.read(fname)

            new_data = data

            # 1) –¥–ª—è word/*.xml –ø—Ä–æ–≥–æ–Ω—è–µ–º –∞–±–∑–∞—Ü—ã —Å id_mapping
            if fname.startswith("word/") and fname.lower().endswith(".xml"):
                new_data = process_docx_xml_paragraphs(new_data, fname, id_mapping)

            # 2) –¥–ª—è –õ–Æ–ë–û–ì–û *.xml ‚Äî –æ–±—â–∞—è –∑–∞–º–µ–Ω–∞ –ø–æ text_mapping
            if fname.lower().endswith(".xml") and text_mapping:
                new_data = replace_georgian_in_xml_bytes(new_data, text_mapping)

            if new_data != data:
                changed += 1

            zout.writestr(info, new_data)

            if progress_callback and total > 0:
                frac = idx / total
                pct = start + (end - start) * frac
                progress_callback(pct, "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ DOCX...")

    print(f"üíæ DOCX —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}, –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö XML: {changed}")


def apply_translations_xlsx(
    input_path: str,
    output_path: str,
    text_mapping: Dict[str, str],
    progress_callback: Optional[Callable[[float, str], None]] = None,
    start: float = 90.0,
    end: float = 100.0,
) -> None:
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø–µ—Ä–µ–≤–æ–¥—ã –∫ XLSX —á–µ—Ä–µ–∑ openpyxl.
    """
    from openpyxl import load_workbook

    wb = load_workbook(input_path, data_only=False)

    total_cells = 0
    for ws in wb.worksheets:
        for row in ws.iter_rows():
            total_cells += len(row)
    if total_cells == 0:
        total_cells = 1

    processed = 0
    changed_cells = 0

    for ws in wb.worksheets:
        for row in ws.iter_rows():
            for cell in row:
                val = cell.value
                if isinstance(val, str):
                    original = val
                    stripped = val.strip()
                    if stripped in text_mapping:
                        new_core = text_mapping[stripped]

                        prefix_len = len(original) - len(original.lstrip())
                        suffix_len = len(original) - len(original.rstrip())
                        prefix = original[:prefix_len]
                        suffix = original[len(original) - suffix_len:] if suffix_len > 0 else ""

                        cell.value = f"{prefix}{new_core}{suffix}"
                        changed_cells += 1

                processed += 1
                if progress_callback:
                    frac = processed / total_cells
                    pct = start + (end - start) * frac
                    progress_callback(pct, "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ XLSX...")

    wb.save(output_path)
    print(f"üíæ XLSX —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}, –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö —è—á–µ–µ–∫: {changed_cells}")


# ============ –ü–µ—Ä–µ–≤–æ–¥—á–∏–∫–∏ (ChatGPT) ============

def translate_with_chatgpt(
    fragments: List[str],
    model_name: str,
    api_key: str,
    target_language: str,
    progress_callback: Optional[Callable[[float, str], None]] = None,
    start: float = 10.0,
    end: float = 60.0,
) -> Dict[str, str]:
    """
    –ü–µ—Ä–µ–≤–æ–¥ –≥—Ä—É–∑–∏–Ω—Å–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ ChatGPT (OpenAI API),
    —Å —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π RateLimitError.
    """
    from openai import OpenAI

    os.environ["OPENAI_API_KEY"] = api_key
    client = OpenAI()

    # –ß–∏—Å—Ç–∏–º –∏ –≤—ã–∫–∏–¥—ã–≤–∞–µ–º –ø—É—Å—Ç–æ–µ
    cleaned = []
    for f in fragments:
        s = (f or "").strip()
        if s:
            cleaned.append(s)

    # –£–Ω–∏–∫–∞–ª–∏–∑–∏—Ä—É–µ–º
    unique_texts: List[str] = []
    seen: Set[str] = set()
    for s in cleaned:
        if s not in seen:
            seen.add(s)
            unique_texts.append(s)

    if not unique_texts:
        return {}

    # –ù–∞–∑–Ω–∞—á–∞–µ–º ID
    id_to_text: Dict[int, str] = {i: txt for i, txt in enumerate(unique_texts)}
    text_to_id: Dict[str, int] = {txt: i for i, txt in id_to_text.items()}

    print(f"–î–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —á–µ—Ä–µ–∑ ChatGPT –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(unique_texts)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.")

    # –ì–æ—Ç–æ–≤–∏–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ {id, text} –¥–ª—è —Ç–æ–∫–µ–Ω–Ω–æ–≥–æ –±–∞—Ç—á–∏–Ω–≥–∞
    fragments_struct: List[Dict[str, Any]] = [
        {"id": i, "text": txt}
        for i, txt in id_to_text.items()
    ]

    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º (–æ—á–µ–Ω—å –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ)
    batches = list(split_fragments_by_tokens(fragments_struct, max_tokens_per_batch=8000))
    print(f"–ë—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {len(batches)} –±–∞—Ç—á(–µ–π) –≤ –º–æ–¥–µ–ª—å {model_name}.")

    total = len(unique_texts)
    done = 0
    id_to_translated: Dict[int, str] = {}

    for batch_idx, batch in enumerate(batches, start=1):
        batch_items = batch  # –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç: {"id": int, "text": str}

        if progress_callback and total > 0:
            frac = done / total
            pct = start + (end - start) * frac
            progress_callback(pct, "–ü–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ ChatGPT...")

        user_payload = {
            "source_language": "Georgian",
            "target_language": target_language,
            "items": batch_items,
        }

        # –¢–í–û–ô system_msg ‚Äî –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        system_msg = (
            "You are a professional legal and technical translator. "
            "The texts are official documents (tariff methodology, regulatory acts, explanatory notes). "
            f"Translate from Georgian to {target_language} into natural, formal, human-quality {target_language}. "
            "You MAY freely change word order, grammar, and morphology so that the result sounds like good native legal language, "
            "but you MUST preserve all facts, numbers, names, and logical relations. "
            "Avoid literal calques from Georgian where they sound unnatural in the target language. "
            "Do NOT merge separate words together: always keep proper spaces between words, "
            "between prepositions and nouns, and around conjunctions. "
            "Fix any missing spaces if they are present in the original. "
            "Return ONLY a JSON object with a single key 'translations', whose value is a list of objects "
            "of the form {\"id\": <same id>, \"text\": <translation>}. "
            "Do not add extra fields."
        )

        # ==== –≤—ã–∑–æ–≤ –º–æ–¥–µ–ª–∏ —Å —Ä–µ—Ç—Ä–∞—è–º–∏ –Ω–∞ RateLimit ====
        max_retries = 5
        delay_seconds = 10

        for attempt in range(1, max_retries + 1):
            try:
                resp = client.chat.completions.create(
                    model=model_name,
                    response_format={"type": "json_object"},
                    messages=[
                        {"role": "system", "content": system_msg},
                        {"role": "user", "content": json.dumps(user_payload, ensure_ascii=False)},
                    ],
                )
                break
            except RateLimitError:
                print(
                    f"[Batch {batch_idx}/{len(batches)}] "
                    f"–ü–µ—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤/–º–∏–Ω—É—Ç—É (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}). "
                    f"–ñ–¥—ë–º {delay_seconds} —Å–µ–∫—É–Ω–¥..."
                )
                if attempt == max_retries:
                    raise
                time.sleep(delay_seconds)
        else:
            # –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ —Å—é–¥–∞ –Ω–µ –¥–æ–π–¥—ë–º (–≤—ã—à–µ raise), –Ω–æ –æ—Å—Ç–∞–≤–∏–º –Ω–∞ –≤—Å—è–∫–∏–π
            raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç ChatGPT –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫.")

        content = resp.choices[0].message.content
        try:
            data = json.loads(content)
        except json.JSONDecodeError:
            print("‚ö†Ô∏è –û—à–∏–±–∫–∞ JSON –æ—Ç –º–æ–¥–µ–ª–∏, –æ—Ç–≤–µ—Ç:")
            print(content)
            raise

        translations_list = data.get("translations")
        if not isinstance(translations_list, list):
            raise ValueError("–û–∂–∏–¥–∞–ª—Å—è –∫–ª—é—á 'translations' —Å–æ —Å–ø–∏—Å–∫–æ–º –æ–±—ä–µ–∫—Ç–æ–≤ {id, text}.")

        for obj in translations_list:
            if not isinstance(obj, dict):
                continue
            tid = obj.get("id")
            ttext = obj.get("text")
            try:
                tid_int = int(tid)
            except (TypeError, ValueError):
                continue
            if not isinstance(ttext, str) or not ttext.strip():
                continue
            id_to_translated[tid_int] = ttext

        done += len(batch_items)
        print(f"   ChatGPT –ø–µ—Ä–µ–≤—ë–ª {done}/{total} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ (batch {batch_idx}/{len(batches)})")

        if progress_callback and total > 0:
            frac = done / total
            pct = start + (end - start) * frac
            progress_callback(pct, "–ü–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ ChatGPT...")

    # –°–æ–±–∏—Ä–∞–µ–º mapping: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç -> –ø–µ—Ä–µ–≤–æ–¥
    mapping: Dict[str, str] = {}
    for txt, tid in text_to_id.items():
        trans = id_to_translated.get(tid)
        if isinstance(trans, str) and trans.strip():
            mapping[txt] = trans
        else:
            mapping[txt] = txt

    return mapping

def post_edit_with_chatgpt(
    mapping: Dict[str, str],
    model_name: str,
    api_key: str,
    target_language: str,
    progress_callback: Optional[Callable[[float, str], None]] = None,
    start: float = 70.0,
    end: float = 90.0,
) -> Dict[str, str]:
    """
    –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ —É–∂–µ –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ ChatGPT,
    —Å –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–º –±–∞—Ç—á–∏–Ω–≥–æ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π RateLimitError.
    """
    from openai import OpenAI

    os.environ["OPENAI_API_KEY"] = api_key
    client = OpenAI()

    unique_values: List[str] = []
    seen = set()
    for v in mapping.values():
        if isinstance(v, str) and v.strip() and v not in seen:
            seen.add(v)
            unique_values.append(v)

    if not unique_values:
        return mapping

    total = len(unique_values)
    done = 0

    # –¥–µ–ª–∏–º –Ω–µ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —à—Ç—É–∫, –∞ –ø–æ –æ—Ü–µ–Ω–æ—á–Ω—ã–º —Ç–æ–∫–µ–Ω–∞–º
    batches = list(split_texts_by_tokens(unique_values, max_tokens_per_batch=8000))
    print(f"[Post-edit] –ë—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {len(batches)} –±–∞—Ç—á(–µ–π) –≤ –º–æ–¥–µ–ª—å {model_name}.")

    improved_map: Dict[str, str] = {}

    system_msg = (
        "You are a professional editor for legal, regulatory and technical documents. "
        f"Improve style, clarity, grammar and fluency in {target_language} while preserving the same meaning, facts, numbers and legal content. "
        "You MAY change word order, fix awkward literal phrases, adjust cases and morphology, "
        "replace unnatural calques with standard legal expressions, and break or merge sentences if it improves readability. "
        "Do NOT add new facts or remove existing ones. "
        "Return ONLY a JSON object mapping each original text to its improved version. "
        "Keys MUST be EXACTLY the original texts. Do not add extra fields."
    )

    max_retries = 5
    delay_seconds = 10

    for batch_idx, batch in enumerate(batches, start=1):
        if progress_callback and total > 0:
            frac = done / total
            pct = start + (end - start) * frac
            progress_callback(pct, "–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ (ChatGPT)...")

        user_payload = {
            "target_language": target_language,
            "texts": batch,
        }

        for attempt in range(1, max_retries + 1):
            try:
                resp = client.chat.completions.create(
                    model=model_name,
                    response_format={"type": "json_object"},
                    messages=[
                        {"role": "system", "content": system_msg},
                        {"role": "user", "content": json.dumps(user_payload, ensure_ascii=False)},
                    ],
                )
                break
            except RateLimitError as e:
                msg = str(e)
                print(
                    f"[Post-edit batch {batch_idx}/{len(batches)}] "
                    f"–ü–µ—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}): {msg}"
                )
                # –µ—Å–ª–∏ –æ—à–∏–±–∫–∞ –≥–æ–≤–æ—Ä–∏—Ç, —á—Ç–æ –∑–∞–ø—Ä–æ—Å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, —Ä–µ—Ç—Ä–∞–∏ –Ω–µ –ø–æ–º–æ–≥—É—Ç
                if "Request too large" in msg and "tokens per min" in msg:
                    raise
                if attempt == max_retries:
                    raise
                time.sleep(delay_seconds)
        else:
            raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç ChatGPT (post_edit) –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫.")

        content = resp.choices[0].message.content
        try:
            data = json.loads(content)
        except json.JSONDecodeError:
            print("‚ö†Ô∏è –û—à–∏–±–∫–∞ JSON –æ—Ç –º–æ–¥–µ–ª–∏ (–≤—ã—á–∏—Ç–∫–∞), –æ—Ç–≤–µ—Ç:")
            print(content)
            raise

        for orig_text in batch:
            new_text = data.get(orig_text)
            if isinstance(new_text, str) and new_text.strip():
                improved_map[orig_text] = new_text
            else:
                improved_map[orig_text] = orig_text

        done += len(batch)
        print(f"   ChatGPT –≤—ã—á–∏—Ç–∞–ª {done}/{total} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ (batch {batch_idx}/{len(batches)})")

        if progress_callback and total > 0:
            frac = done / total
            pct = start + (end - start) * frac
            progress_callback(pct, "–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ (ChatGPT)...")

    # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–π mapping: –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π -> —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥
    new_mapping: Dict[str, str] = {}
    for geo, raw_trans in mapping.items():
        if isinstance(raw_trans, str):
            new_mapping[geo] = improved_map.get(raw_trans, raw_trans)
        else:
            new_mapping[geo] = raw_trans

    return new_mapping



def fix_spacing_with_chatgpt(
    mapping: Dict[str, str],
    model_name: str,
    api_key: str,
    progress_callback: Optional[Callable[[float, str], None]] = None,
    start: float = 70.0,
    end: float = 90.0,
) -> Dict[str, str]:
    """
    –ê–∫–∫—É—Ä–∞—Ç–Ω–∞—è –ø—Ä–∞–≤–∫–∞ –ü–†–û–ë–ï–õ–û–í –≤ —É–∂–µ –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω–æ–º —Ä—É—Å—Å–∫–æ–º —Ç–µ–∫—Å—Ç–µ,
    —Å –±–∞—Ç—á–∏–Ω–≥–æ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π RateLimitError.
    """
    from openai import OpenAI

    os.environ["OPENAI_API_KEY"] = api_key
    client = OpenAI()

    unique_values: List[str] = []
    seen = set()
    for v in mapping.values():
        if isinstance(v, str) and v.strip() and v not in seen:
            seen.add(v)
            unique_values.append(v)

    if not unique_values:
        return mapping

    total = len(unique_values)
    done = 0

    batches = list(split_texts_by_tokens(unique_values, max_tokens_per_batch=8000))
    print(f"[Fix-spacing] –ë—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {len(batches)} –±–∞—Ç—á(–µ–π) –≤ –º–æ–¥–µ–ª—å {model_name}.")

    fixed_map: Dict[str, str] = {}

    system_msg = (
        "You receive Russian texts which are already translated correctly. "
        "Your ONLY task is to fix spacing errors: insert or delete ASCII space characters (U+0020) "
        "where necessary between words, numbers and punctuation, and collapse multiple spaces to single ones if appropriate. "
        "You MUST NOT change, delete, reorder or insert ANY non-space characters (letters, digits, punctuation). "
        "Return ONLY a JSON object mapping each original string to its corrected version. "
        "Keys MUST be EXACTLY the original strings. Do not add extra fields."
    )

    max_retries = 5
    delay_seconds = 10

    for batch_idx, batch in enumerate(batches, start=1):
        if progress_callback and total > 0:
            frac = done / total
            pct = start + (end - start) * frac
            progress_callback(pct, "–ü—Ä–∞–≤–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ —Ä—É—Å—Å–∫–æ–º —Ç–µ–∫—Å—Ç–µ...")

        user_payload = {
            "texts": batch,
        }

        for attempt in range(1, max_retries + 1):
            try:
                resp = client.chat.completions.create(
                    model=model_name,
                    response_format={"type": "json_object"},
                    messages=[
                        {"role": "system", "content": system_msg},
                        {"role": "user", "content": json.dumps(user_payload, ensure_ascii=False)},
                    ],
                )
                break
            except RateLimitError as e:
                msg = str(e)
                print(
                    f"[Fix-spacing batch {batch_idx}/{len(batches)}] "
                    f"–ü–µ—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}): {msg}"
                )
                if "Request too large" in msg and "tokens per min" in msg:
                    raise
                if attempt == max_retries:
                    raise
                time.sleep(delay_seconds)
        else:
            raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç ChatGPT (fix_spacing) –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫.")

        content = resp.choices[0].message.content
        try:
            data = json.loads(content)
        except json.JSONDecodeError:
            print("‚ö†Ô∏è –û—à–∏–±–∫–∞ JSON –æ—Ç –º–æ–¥–µ–ª–∏ (fix_spacing), –æ—Ç–≤–µ—Ç:")
            print(content)
            raise

        for orig_text in batch:
            new_text = data.get(orig_text)
            if isinstance(new_text, str) and new_text.strip():
                fixed_map[orig_text] = new_text
            else:
                fixed_map[orig_text] = orig_text

        done += len(batch)
        print(f"   ChatGPT –ø–æ–ø—Ä–∞–≤–∏–ª –ø—Ä–æ–±–µ–ª—ã –≤ {done}/{total} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö (batch {batch_idx}/{len(batches)})")

        if progress_callback and total > 0:
            frac = done / total
            pct = start + (end - start) * frac
            progress_callback(pct, "–ü—Ä–∞–≤–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ —Ä—É—Å—Å–∫–æ–º —Ç–µ–∫—Å—Ç–µ...")

    new_mapping: Dict[str, str] = {}
    for geo, ru in mapping.items():
        if isinstance(ru, str):
            new_mapping[geo] = fixed_map.get(ru, ru)
        else:
            new_mapping[geo] = ru

    return new_mapping



# ============ –ü–µ—Ä–µ–≤–æ–¥—á–∏–∫ NLLB (–ª–æ–∫–∞–ª—å–Ω—ã–π) ============

def translate_with_local_model(
    fragments: List[str],
    direction_code: str,
    progress_callback: Optional[Callable[[float, str], None]] = None,
    start: float = 10.0,
    end: float = 90.0,
) -> Dict[str, str]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –õ–û–ö–ê–õ–¨–ù–´–ô –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ NLLB-200.
    """

    LANG_MAP = {
        "ka": "kat_Geor",
        "ru": "rus_Cyrl",
        "en": "eng_Latn",
    }

    if "-" not in direction_code:
        raise ValueError(f"direction_code –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ñ–æ—Ä–º–∞—Ç–∞ ka-ru, –∞ –ø–æ–ª—É—á–µ–Ω–æ: {direction_code}")

    src, tgt = direction_code.split("-")

    if src not in LANG_MAP:
        raise ValueError(f"–ò—Å—Ç–æ—á–Ω–∏–∫ —è–∑—ã–∫–∞ '{src}' –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∞–Ω –≤ LANG_MAP")

    if tgt not in LANG_MAP:
        raise ValueError(f"–¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫ '{tgt}' –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∞–Ω –≤ LANG_MAP")

    SRC_LANG = LANG_MAP[src]
    TGT_LANG = LANG_MAP[tgt]

    remaining = [f.strip() for f in fragments if isinstance(f, str) and f.strip()]
    if not remaining:
        return {}

    MODEL_NAME = "facebook/nllb-200-3.3B"

    print(f"‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é NLLB –º–æ–¥–µ–ª—å: {MODEL_NAME}")
    if progress_callback:
        progress_callback(start, f"–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ NLLB ({MODEL_NAME})‚Ä¶")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, src_lang=SRC_LANG)
    device = torch.device("cpu")
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)
    model.eval()

    BATCH_SIZE = 1
    MAX_TOKENS = 256

    total = len(remaining)
    done = 0
    mapping: Dict[str, str] = {}

    for i, batch in enumerate(chunks(remaining, BATCH_SIZE), start=1):
        print(f"--> [NLLB {direction_code}] batch {i}, size={len(batch)}")

        inputs = tokenizer(
            batch,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512,
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}

        bos_id = tokenizer.convert_tokens_to_ids(TGT_LANG)
        if bos_id is None:
            raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å token id –¥–ª—è —è–∑—ã–∫–∞ {TGT_LANG}")

        with torch.no_grad():
            generated = model.generate(
                **inputs,
                forced_bos_token_id=bos_id,
                max_length=MAX_TOKENS,
            )

        outputs = tokenizer.batch_decode(generated, skip_special_tokens=True)

        for orig, trans in zip(batch, outputs):
            trans = (trans or "").strip()
            mapping[orig] = trans if trans else orig

        done += len(batch)
        if progress_callback and total > 0:
            pct = start + (end - start) * (done / total)
            progress_callback(pct, f"–ü–µ—Ä–µ–≤–æ–¥ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é (NLLB, {direction_code})‚Ä¶")

        print(f"   [NLLB {direction_code}] –≥–æ—Ç–æ–≤–æ {done}/{total}")

    return mapping


# ============ –õ–æ–∫–∞–ª—å–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ (Qwen) ============

def post_edit_with_qwen_local(
    mapping: Dict[str, str],
    target_language: str,
    progress_callback: Optional[Callable[[float, str], None]] = None,
    start: float = 60.0,
    end: float = 90.0,
) -> Dict[str, str]:
    """
    –õ–æ–∫–∞–ª—å–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é Qwen2.5-3B-Instruct.

    mapping: {–≥—Ä—É–∑–∏–Ω—Å–∫–∏–π_–æ—Ä–∏–≥–∏–Ω–∞–ª -> –º–∞—à–∏–Ω–Ω—ã–π_–ø–µ—Ä–µ–≤–æ–¥ (ru/en)}
    target_language: "Russian" / "English" (–∏–∑ DIRECTION_CONFIG)
    """
    MODEL_NAME = "Qwen/Qwen2.5-3B-Instruct"

    # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    unique_values: List[str] = []
    seen = set()
    for v in mapping.values():
        if isinstance(v, str) and v.strip() and v not in seen:
            seen.add(v)
            unique_values.append(v)

    if not unique_values:
        return mapping

    print(f"‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –≤—ã—á–∏—Ç–∫–∏: {MODEL_NAME}")
    if progress_callback:
        progress_callback(start, f"–ó–∞–≥—Ä—É–∑–∫–∞ Qwen2.5-3B-Instruct –¥–ª—è –≤—ã—á–∏—Ç–∫–∏‚Ä¶")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if device.type == "cuda":
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16,
            device_map="auto",
        )
    else:
        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)
    model.eval()

    system_msg_text = (
        f"You are a professional editor for {target_language} legal, regulatory and technical documents. "
        f"Improve style, clarity, grammar and fluency in {target_language} while preserving the same meaning, "
        "facts, numbers and legal content. "
        "You MAY change word order, fix awkward literal phrases, adjust cases and morphology, "
        "replace unnatural calques with standard legal expressions, and break or merge sentences if it improves readability. "
        "Do NOT add new facts or remove existing ones. "
        "Return ONLY the improved text, without explanations, without quotes."
    )

    BATCH_SIZE = 4
    total = len(unique_values)
    done = 0

    improved_map: Dict[str, str] = {}

    for batch in chunks(unique_values, BATCH_SIZE):
        for text in batch:
            # Chat-—à–∞–±–ª–æ–Ω –¥–ª—è Qwen
            messages = [
                {"role": "system", "content": system_msg_text},
                {"role": "user", "content": text},
            ]
            prompt = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
            )

            inputs = tokenizer(prompt, return_tensors="pt").to(device)

            with torch.no_grad():
                output_ids = model.generate(
                    **inputs,
                    max_new_tokens=512,
                    do_sample=False,
                )

            # –û–±—Ä–µ–∑–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å-–ø—Ä–æ–º–ø—Ç –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫—É—Å–æ–∫
            gen_ids = output_ids[0][inputs["input_ids"].shape[1]:]
            out_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()

            if not out_text:
                out_text = text

            improved_map[text] = out_text

        done += len(batch)
        if progress_callback and total > 0:
            frac = done / total
            pct = start + (end - start) * frac
            progress_callback(pct, "–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ (–ª–æ–∫–∞–ª—å–Ω–∞—è Qwen)‚Ä¶")

        print(f"   Qwen –≤—ã—á–∏—Ç–∞–ª {done}/{total} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")

    # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–π mapping: –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π -> —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥
    new_mapping: Dict[str, str] = {}
    for geo, raw_trans in mapping.items():
        if isinstance(raw_trans, str):
            new_mapping[geo] = improved_map.get(raw_trans, raw_trans)
        else:
            new_mapping[geo] = raw_trans

    return new_mapping


# ============ –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–∞–≤–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤ (—Ä–µ–≥—ç–∫—Å—ã) ============

def normalize_segment_boundaries(segments: List[str]) -> List[str]:
    if len(segments) <= 1:
        return segments

    segs = segments[:]

    for i in range(len(segs) - 1):
        left = segs[i]
        right = segs[i + 1]

        left = re.sub(r' {2,}', ' ', left)
        right = re.sub(r' {2,}', ' ', right)

        if left.endswith(" ") and right.startswith(" "):
            right = right.lstrip()

        segs[i] = left
        segs[i + 1] = right

    return segs


def fix_basic_spacing_ru(text: str) -> str:
    import re

    text = text.replace("\u00A0", " ")

    text = re.sub(r'([–ê-–Ø–Å–∞-—è—ë])‚Ññ\s*(\d)', r'\1 ‚Ññ\2', text)
    text = re.sub(r'‚Ññ\s*(\d)', r'‚Ññ \1', text)

    text = re.sub(r'(?i)\b(–æ—Ç|–¥–æ|–ø–æ|–Ω–∞|–≤|–∫|—Å|—É)(\d)', r'\1 \2', text)

    text = re.sub(r'(\d)([–ê-–Ø–Å–∞-—è—ë])', r'\1 \2', text)

    text = re.sub(r'(\d{3,4})\s*(–≥–æ–¥[–∞—É–µ]?|–≥–≥?\.?)', r'\1 \2', text)

    text = re.sub(r',([^\s])', r', \1', text)
    text = re.sub(r';([^\s])', r'; \1', text)
    text = re.sub(r'([^.])\.([–ê-–Ø–Å])', r'\1. \2', text)

    text = re.sub(r'(%)([–ê-–Ø–Å–∞-—è—ë])', r'\1 \2', text)

    text = re.sub(r'[ \t]{2,}', ' ', text)

    return text


# ============ –õ–æ–≥–∏–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ ============

def process_file(
    file_path: str,
    translator_kind: str,
    chatgpt_model: str,
    env_path: Optional[str],
    direction_code: str,
    post_edit: bool,
    progress_callback: Optional[Callable[[float, str], None]] = None,
) -> str:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: —Å–æ–±–∏—Ä–∞–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –≤—ã–±—Ä–∞–Ω–Ω—ã–º –¥–≤–∏–∂–∫–æ–º,
    –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–µ–ª–∞–µ—Ç –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—É—é –≤—ã—á–∏—Ç–∫—É,
    –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø–µ—Ä–µ–≤–æ–¥—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É.
    """

    if progress_callback is None:
        def progress_callback(pct: float, msg: str) -> None:
            pass  # –∑–∞–≥–ª—É—à–∫–∞

    if not (is_docx(file_path) or is_xlsx(file_path)):
        raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã .docx –∏ .xlsx")

    if direction_code not in DIRECTION_CONFIG:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {direction_code}")

    meta = DIRECTION_CONFIG[direction_code]
    target_language = meta["target_language"]
    suffix = meta["suffix"]

    # 1. –°–±–æ—Ä —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
    progress_callback(0.0, "–°–±–æ—Ä –≥—Ä—É–∑–∏–Ω—Å–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")

    if is_docx(file_path):
        items = collect_docx_items(file_path)
        if not items:
            progress_callback(0.0, "–ì—Ä—É–∑–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            raise RuntimeError("–í —Ñ–∞–π–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞.")

        base_texts = {str(it["clean_text"]) for it in items}

        extra_texts: Set[str] = set()
        with zipfile.ZipFile(file_path, "r") as zin:
            for info in zin.infolist():
                fname = info.filename
                if not fname.lower().endswith(".xml"):
                    continue

                xml_bytes = zin.read(fname)
                extra_texts.update(collect_georgian_fragments_from_xml_bytes(xml_bytes))

        all_texts = base_texts | extra_texts

        fragments_for_translation = sorted(
            t for t in all_texts
            if t.strip() and GEORGIAN_RE.search(t)
        )

        items_for_docx = items
    else:
        fragments_set = collect_fragments_xlsx(file_path)
        if not fragments_set:
            progress_callback(0.0, "–ì—Ä—É–∑–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            raise RuntimeError("–í —Ñ–∞–π–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞.")
        fragments_for_translation = sorted(fragments_set)
        items_for_docx = None

    print(f"–ù–∞–π–¥–µ–Ω–æ {len(fragments_for_translation)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞.")
    progress_callback(5.0, f"–ù–∞–π–¥–µ–Ω–æ {len(fragments_for_translation)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –ø–µ—Ä–µ–≤–æ–¥—É...")

    # 2. –ü–µ—Ä–µ–≤–æ–¥
    if translator_kind == "chatgpt":
        if not env_path:
            raise ValueError("–ù–µ –≤—ã–±—Ä–∞–Ω .env —Ñ–∞–π–ª —Å —Ç–æ–∫–µ–Ω–æ–º –¥–ª—è ChatGPT.")
        api_key = load_api_key_from_env_file(env_path)

        mapping_text_to_trans = translate_with_chatgpt(
            fragments_for_translation,
            chatgpt_model,
            api_key,
            target_language,
            progress_callback=progress_callback,
            start=10.0,
            end=60.0,
        )

        if post_edit:
            mapping_text_to_trans = post_edit_with_chatgpt(
                mapping_text_to_trans,
                chatgpt_model,
                api_key,
                target_language,
                progress_callback=progress_callback,
                start=60.0,
                end=90.0,
            )

    else:
        # –õ–æ–∫–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ NLLB
        mapping_text_to_trans = translate_with_local_model(
            fragments_for_translation,
            direction_code,
            progress_callback=progress_callback,
            start=10.0,
            end=60.0,
        )

        # –õ–æ–∫–∞–ª—å–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ Qwen (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞)
        if post_edit:
            mapping_text_to_trans = post_edit_with_qwen_local(
                mapping_text_to_trans,
                target_language=target_language,
                progress_callback=progress_callback,
                start=60.0,
                end=90.0,
            )

    # 2b. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è DOCX –≤ —Ñ–æ—Ä–º–∞—Ç id -> translation
    if is_docx(file_path):
        id_mapping: Dict[str, str] = {}
        for it in items_for_docx:  # type: ignore
            clean_text = str(it["clean_text"])
            item_id = str(it["id"])
            translated = mapping_text_to_trans.get(clean_text, clean_text)
            id_mapping[item_id] = translated
    else:
        id_mapping = mapping_text_to_trans

    # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã –∫ —Ñ–∞–π–ª—É
    base, ext = os.path.splitext(file_path)
    output_path = f"{base}{suffix}{ext}"

    progress_callback(90.0, "–ü—Ä–∏–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã –∫ —Ñ–∞–π–ª—É...")
    if is_docx(file_path):
        apply_translations_docx(
            file_path,
            output_path,
            id_mapping,
            mapping_text_to_trans,
            progress_callback=progress_callback,
            start=90.0,
            end=100.0,
        )
    else:
        apply_translations_xlsx(
            file_path,
            output_path,
            id_mapping,
            progress_callback=progress_callback,
            start=90.0,
            end=100.0,
        )

    progress_callback(100.0, "–ì–æ—Ç–æ–≤–æ.")
    if is_docx(output_path):
        debug_scan_docx_for_georgian(output_path)

    return output_path


# ============ GUI (Tkinter) ============

class TranslatorGUI:
    def __init__(self, root: tk.Tk):
        self.root = root
        self.root.title("–ü–µ—Ä–µ–≤–æ–¥ –≥—Ä—É–∑–∏–Ω—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ DOCX/XLSX")

        self.file_path_var = tk.StringVar()
        self.env_path_var = tk.StringVar()
        self.translator_var = tk.StringVar(value="chatgpt")
        self.model_var = tk.StringVar(value=DEFAULT_CHATGPT_MODEL)
        self.direction_label_var = tk.StringVar(value=DIRECTION_CONFIG["ka-ru"]["label"])

        self.progress_var = tk.DoubleVar(value=0.0)
        self.status_var = tk.StringVar(value="–ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ.")

        self.post_edit_var = tk.BooleanVar(value=False)

        self.start_button: Optional[ttk.Button] = None
        self.post_edit_check: Optional[ttk.Checkbutton] = None

        self.build_ui()

    def build_ui(self):
        pad = 5

        frm = ttk.Frame(self.root, padding=10)
        frm.grid(row=0, column=0, sticky="nsew")

        ttk.Label(frm, text="–§–∞–π–ª DOCX/XLSX:").grid(row=0, column=0, sticky="w", pady=pad)
        entry_file = ttk.Entry(frm, textvariable=self.file_path_var, width=60)
        entry_file.grid(row=0, column=1, sticky="we", pady=pad)
        ttk.Button(frm, text="–í—ã–±—Ä–∞—Ç—å...", command=self.choose_file).grid(row=0, column=2, padx=pad, pady=pad)

        ttk.Label(frm, text="–ü–µ—Ä–µ–≤–æ–¥—á–∏–∫:").grid(row=1, column=0, sticky="w", pady=pad)

        r1 = ttk.Radiobutton(
            frm,
            text="ChatGPT (–æ–±–ª–∞—á–Ω—ã–π)",
            variable=self.translator_var,
            value="chatgpt",
            command=self.on_translator_change,
        )
        r1.grid(row=1, column=1, sticky="w", pady=pad)

        r2 = ttk.Radiobutton(
            frm,
            text="–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (NLLB-200)",
            variable=self.translator_var,
            value="local",
            command=self.on_translator_change,
        )
        r2.grid(row=2, column=1, sticky="w", pady=pad)

        ttk.Label(frm, text="–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞:").grid(row=3, column=0, sticky="w", pady=pad)
        direction_values = [meta["label"] for meta in DIRECTION_CONFIG.values()]
        self.direction_combo = ttk.Combobox(
            frm,
            textvariable=self.direction_label_var,
            values=direction_values,
            state="readonly",
            width=35,
        )
        self.direction_combo.grid(row=3, column=1, sticky="w", pady=pad)

        ttk.Label(frm, text="–ú–æ–¥–µ–ª—å ChatGPT:").grid(row=4, column=0, sticky="w", pady=pad)
        self.model_combo = ttk.Combobox(
            frm,
            textvariable=self.model_var,
            values=CHATGPT_MODELS,
            state="readonly",
            width=35,
        )
        self.model_combo.grid(row=4, column=1, sticky="w", pady=pad)

        ttk.Label(frm, text=".env —Å —Ç–æ–∫–µ–Ω–æ–º:").grid(row=5, column=0, sticky="w", pady=pad)
        self.env_entry = ttk.Entry(frm, textvariable=self.env_path_var, width=60)
        self.env_entry.grid(row=5, column=1, sticky="we", pady=pad)
        self.env_button = ttk.Button(frm, text="–í—ã–±—Ä–∞—Ç—å .env...", command=self.choose_env_file)
        self.env_button.grid(row=5, column=2, padx=pad, pady=pad)

        self.post_edit_check = ttk.Checkbutton(
            frm,
            text="–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ (—É–ª—É—á—à–∞—Ç—å —Å—Ç–∏–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞)",
            variable=self.post_edit_var,
        )
        self.post_edit_check.grid(row=6, column=0, columnspan=3, sticky="w", pady=pad)

        ttk.Label(frm, text="–ü—Ä–æ–≥—Ä–µ—Å—Å:").grid(row=7, column=0, sticky="w", pady=pad)
        self.progress_bar = ttk.Progressbar(
            frm,
            maximum=100.0,
            variable=self.progress_var,
            mode="determinate",
            length=300,
        )
        self.progress_bar.grid(row=7, column=1, columnspan=2, sticky="we", pady=pad)

        self.status_label = ttk.Label(frm, textvariable=self.status_var)
        self.status_label.grid(row=8, column=0, columnspan=3, sticky="w", pady=pad)

        self.start_button = ttk.Button(frm, text="–°—Ç–∞—Ä—Ç –ø–µ—Ä–µ–≤–æ–¥–∞", command=self.run_translation)
        self.start_button.grid(row=9, column=0, columnspan=3, pady=10)

        self.root.columnconfigure(0, weight=1)
        frm.columnconfigure(1, weight=1)

        self.on_translator_change()

    def _update_progress_mainthread(self, pct: float, msg: str) -> None:
        self.progress_var.set(max(0.0, min(100.0, pct)))
        self.status_var.set(msg)

    def set_progress(self, pct: float, msg: str) -> None:
        self.root.after(0, self._update_progress_mainthread, pct, msg)

    def choose_file(self):
        path = filedialog.askopenfilename(
            title="–í—ã–±–µ—Ä–∏—Ç–µ DOCX –∏–ª–∏ XLSX",
            filetypes=[("Office files", "*.docx *.xlsx"), ("–í—Å–µ —Ñ–∞–π–ª—ã", "*.*")],
        )
        if path:
            self.file_path_var.set(path)

    def choose_env_file(self):
        path = filedialog.askopenfilename(
            title="–í—ã–±–µ—Ä–∏—Ç–µ .env —Ñ–∞–π–ª —Å OPENAI_API_KEY",
            filetypes=[("ENV files", "*.env;*.txt;*.*"), ("–í—Å–µ —Ñ–∞–π–ª—ã", "*.*")],
        )
        if path:
            self.env_path_var.set(path)

    def on_translator_change(self):
        kind = self.translator_var.get()
        if kind == "chatgpt":
            self.model_combo.configure(state="readonly")
            self.env_entry.configure(state="normal")
            self.env_button.configure(state="normal")
            if self.post_edit_check is not None:
                self.post_edit_check.configure(
                    text="–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ (—á–µ—Ä–µ–∑ ChatGPT)",
                    state="normal",
                )
        else:
            self.model_combo.configure(state="disabled")
            self.env_entry.configure(state="disabled")
            self.env_button.configure(state="disabled")
            if self.post_edit_check is not None:
                self.post_edit_check.configure(
                    text="–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ (–ª–æ–∫–∞–ª—å–Ω–∞—è Qwen2.5-3B-Instruct)",
                    state="normal",
                )

    def run_translation(self):
        file_path = self.file_path_var.get().strip()
        if not file_path:
            messagebox.showerror("–û—à–∏–±–∫–∞", "–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª DOCX/XLSX.")
            return

        if not (is_docx(file_path) or is_xlsx(file_path)):
            messagebox.showerror("–û—à–∏–±–∫–∞", "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã .docx –∏ .xlsx.")
            return

        translator_kind = self.translator_var.get()
        chatgpt_model = self.model_var.get()
        env_path = self.env_path_var.get().strip() if translator_kind == "chatgpt" else None
        post_edit = bool(self.post_edit_var.get())

        direction_label = self.direction_label_var.get()
        try:
            direction_code = get_direction_code_from_label(direction_label)
        except ValueError as e:
            messagebox.showerror("–û—à–∏–±–∫–∞", str(e))
            return

        if translator_kind == "chatgpt" and not env_path:
            messagebox.showerror("–û—à–∏–±–∫–∞", "–í—ã–±–µ—Ä–∏—Ç–µ .env —Ñ–∞–π–ª —Å —Ç–æ–∫–µ–Ω–æ–º –¥–ª—è ChatGPT.")
            return

        self.start_button.configure(state="disabled")
        self.set_progress(0.0, "–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏...")

        t = threading.Thread(
            target=self._worker_translate,
            args=(file_path, translator_kind, chatgpt_model, env_path, direction_code, post_edit),
            daemon=True,
        )
        t.start()

    def _worker_translate(self, file_path: str, translator_kind: str,
                          chatgpt_model: str, env_path: Optional[str],
                          direction_code: str, post_edit: bool):
        try:
            output_path = process_file(
                file_path=file_path,
                translator_kind=translator_kind,
                chatgpt_model=chatgpt_model,
                env_path=env_path,
                direction_code=direction_code,
                post_edit=post_edit,
                progress_callback=self.set_progress,
            )
        except Exception as e:
            import traceback
            traceback.print_exc()

            err_msg = f"{type(e).__name__}: {e}"

            def show_error(msg=err_msg):
                self.start_button.configure(state="normal")
                self._update_progress_mainthread(0.0, "–û—à–∏–±–∫–∞.")
                messagebox.showerror("–û—à–∏–±–∫–∞", f"–ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π:\n{msg}")

            self.root.after(0, show_error)
            return

        def on_done():
            self.start_button.configure(state="normal")
            self._update_progress_mainthread(100.0, "–ì–æ—Ç–æ–≤–æ.")
            messagebox.showinfo("–ì–æ—Ç–æ–≤–æ", f"–§–∞–π–ª –ø–µ—Ä–µ–≤–µ–¥—ë–Ω:\n{output_path}")
        self.root.after(0, on_done)


def main():
    root = tk.Tk()
    TranslatorGUI(root)
    root.mainloop()


if __name__ == "__main__":
    main()
