import os
import re
import json
import time
import zipfile
import threading
import tkinter as tk
from tkinter import filedialog, messagebox
from tkinter import ttk
from typing import Dict, List, Set, Iterable, Callable, Optional, Any, Tuple

import xml.etree.ElementTree as ET

from openai import RateLimitError
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM
import torch


# =============================================================================
# HuggingFace ONLINE/OFFLINE helpers (–ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏ –±–µ–∑ –¥—É–±–ª–µ–π)
# =============================================================================

def hf_enable_online() -> None:
    """–†–∞–∑—Ä–µ—à–∏—Ç—å HuggingFace/transformers —Ö–æ–¥–∏—Ç—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç (—Å–Ω—è—Ç—å offline-—Ñ–ª–∞–≥–∏)."""
    os.environ.pop("HF_HUB_OFFLINE", None)
    os.environ.pop("TRANSFORMERS_OFFLINE", None)


def hf_enable_offline() -> None:
    """–ó–∞–ø—Ä–µ—Ç–∏—Ç—å HuggingFace/transformers —Ö–æ–¥–∏—Ç—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç."""
    os.environ["HF_HUB_OFFLINE"] = "1"
    os.environ["TRANSFORMERS_OFFLINE"] = "1"
    os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"


def hf_print_mode(prefix: str = "") -> None:
    """–î–ª—è –æ—Ç–ª–∞–¥–∫–∏."""
    print(
        f"{prefix}HF_HUB_OFFLINE={os.getenv('HF_HUB_OFFLINE')} "
        f"TRANSFORMERS_OFFLINE={os.getenv('TRANSFORMERS_OFFLINE')}"
    )


# =============================================================================
# Token split (–¥–ª—è ChatGPT –±–∞—Ç—á–µ–π)
# =============================================================================

def estimate_tokens(text: str) -> int:
    """–ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤."""
    if not text:
        return 1
    return int(len(text) * 1.2)  # +20% –∑–∞–ø–∞—Å —Å–≤–µ—Ä—Ö—É


def split_fragments_by_tokens(
    fragments: List[Dict[str, Any]],
    max_tokens_per_batch: int = 8000,
) -> Iterable[List[Dict[str, Any]]]:
    batch: List[Dict[str, Any]] = []
    current_tokens = 0

    for frag in fragments:
        text = frag["text"]
        t = estimate_tokens(text)

        if t > max_tokens_per_batch:
            if batch:
                yield batch
                batch = []
                current_tokens = 0
            yield [frag]
            continue

        if batch and current_tokens + t > max_tokens_per_batch:
            yield batch
            batch = []
            current_tokens = 0

        batch.append(frag)
        current_tokens += t

    if batch:
        yield batch


def split_texts_by_tokens(
    texts: List[str],
    max_tokens_per_batch: int = 8000,
) -> Iterable[List[str]]:
    batch: List[str] = []
    current_tokens = 0

    for text in texts:
        if not isinstance(text, str):
            continue
        t = estimate_tokens(text)

        if t > max_tokens_per_batch:
            if batch:
                yield batch
                batch = []
                current_tokens = 0
            yield [text]
            continue

        if batch and current_tokens + t > max_tokens_per_batch:
            yield batch
            batch = []
            current_tokens = 0

        batch.append(text)
        current_tokens += t

    if batch:
        yield batch


# =============================================================================
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
# =============================================================================

DEFAULT_CHATGPT_MODEL = "gpt-4.1-mini"
CHATGPT_MODELS = [
    "gpt-4.1-mini",
    "gpt-4.1",
    "gpt-4o-mini",
    "gpt-4o",
    "gpt-5.1",
    "gpt-5-mini",
]

DIRECTION_CONFIG: Dict[str, Dict[str, str]] = {
    "ka-ru": {"label": "–ì—Ä—É–∑–∏–Ω—Å–∫–∏–π ‚Üí –†—É—Å—Å–∫–∏–π", "target_language": "Russian", "suffix": "_ru"},
    "ka-en": {"label": "–ì—Ä—É–∑–∏–Ω—Å–∫–∏–π ‚Üí –ê–Ω–≥–ª–∏–π—Å–∫–∏–π", "target_language": "English", "suffix": "_en"},
}

# –¥–∏–∞–ø–∞–∑–æ–Ω—ã Unicode –¥–ª—è –≥—Ä—É–∑–∏–Ω—Å–∫–æ–≥–æ
GEORGIAN_RE = re.compile(r"[\u10A0-\u10FF\u1C90-\u1CBF]+")


# =============================================================================
# –£—Ç–∏–ª–∏—Ç—ã
# =============================================================================

def is_docx(path: str) -> bool:
    return path.lower().endswith(".docx")


def is_xlsx(path: str) -> bool:
    return path.lower().endswith(".xlsx")


def is_pptx(path: str) -> bool:
    return path.lower().endswith(".pptx")


def chunks(lst: List[str], n: int) -> Iterable[List[str]]:
    for i in range(0, len(lst), n):
        yield lst[i:i + n]


def load_api_key_from_env_file(env_path: str) -> str:
    """
    –ß–∏—Ç–∞–µ—Ç .env-—Ñ–∞–π–ª –∏ –∏—â–µ—Ç OPENAI_API_KEY / API_KEY.
    –ï—Å–ª–∏ –Ω–µ –Ω–∞—à—ë–ª ‚Äî –±–µ—Ä—ë—Ç –ø–µ—Ä–≤—É—é –Ω–µ–ø—É—Å—Ç—É—é, –Ω–µ–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É –∫–∞–∫ –∫–ª—é—á.
    """
    if not os.path.exists(env_path):
        raise FileNotFoundError(f".env —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {env_path}")

    candidate = None
    with open(env_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            if "=" in line:
                k, v = line.split("=", 1)
                k = k.strip()
                v = v.strip()
                if k in ("OPENAI_API_KEY", "API_KEY"):
                    return v
                if candidate is None and v:
                    candidate = v
            else:
                if candidate is None:
                    candidate = line

    if not candidate:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–ª—é—á API –∏–∑ .env —Ñ–∞–π–ª–∞.")
    return candidate


def get_direction_code_from_label(label: str) -> str:
    for code, meta in DIRECTION_CONFIG.items():
        if meta["label"] == label:
            return code
    raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞: {label}")


# =============================================================================
# –†–∞–±–æ—Ç–∞ —Å XML –≤–Ω—É—Ç—Ä–∏ DOCX/XLSX/PPTX
# =============================================================================

def collect_docx_items(path: str) -> List[Dict[str, object]]:
    """
    DOCX ‚Äî —Å–æ–±–∏—Ä–∞–µ–º –∞–±–∑–∞—Ü—ã (<w:p>), –≥–¥–µ –µ—Å—Ç—å –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å id –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞.
    """
    items: List[Dict[str, object]] = []

    with zipfile.ZipFile(path, "r") as zin:
        for info in zin.infolist():
            fname = info.filename
            if not (fname.startswith("word/") and fname.lower().endswith(".xml")):
                continue

            xml_bytes = zin.read(fname)
            try:
                root = ET.fromstring(xml_bytes)
            except Exception:
                continue

            m = re.match(r"\{(.*)\}", root.tag)
            ns = m.group(1) if m else ""
            p_tag = f"{{{ns}}}p"
            t_tag = f"{{{ns}}}t"

            for p_index, p in enumerate(root.iter(p_tag)):
                t_elems = list(p.iter(t_tag))
                if not t_elems:
                    continue

                parts = [(t.text or "") for t in t_elems]
                full_text = "".join(parts)
                if not full_text:
                    continue
                if not GEORGIAN_RE.search(full_text):
                    continue

                clean_text = full_text.strip()
                if not clean_text:
                    continue

                item_id = f"{fname}::p{p_index}"
                items.append({
                    "id": item_id,
                    "xml_name": fname,
                    "p_index": p_index,
                    "full_text": full_text,
                    "clean_text": clean_text,
                })

    print(f"üìÑ DOCX: –Ω–∞–π–¥–µ–Ω–æ {len(items)} –∞–±–∑–∞—Ü–µ–≤ —Å –≥—Ä—É–∑–∏–Ω—Å–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º.")
    return items


def collect_georgian_fragments_from_xml_bytes(xml_bytes: bytes) -> Set[str]:
    """–ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —É–∑–ª—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π, –∏ –±–µ—Ä—ë–º –∏—Ö strip()."""
    result: Set[str] = set()
    try:
        root = ET.fromstring(xml_bytes)
    except Exception:
        return result

    for elem in root.iter():
        text = elem.text
        if not text:
            continue
        if GEORGIAN_RE.search(text):
            cleaned = text.strip()
            if cleaned:
                result.add(cleaned)
    return result


def collect_fragments_xlsx(path: str) -> Set[str]:
    """XLSX ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º sharedStrings + worksheets."""
    to_translate: Set[str] = set()

    with zipfile.ZipFile(path, "r") as zin:
        for info in zin.infolist():
            fname = info.filename.lower()

            if fname.startswith("xl/sharedstrings") and fname.endswith(".xml"):
                xml_bytes = zin.read(info.filename)
            elif fname.startswith("xl/worksheets/") and fname.endswith(".xml"):
                xml_bytes = zin.read(info.filename)
            else:
                continue

            frags = collect_georgian_fragments_from_xml_bytes(xml_bytes)
            to_translate.update(frags)

    print(f"üìä XLSX: –Ω–∞–π–¥–µ–Ω–æ {len(to_translate)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥—Ä—É–∑–∏–Ω—Å–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.")
    return to_translate


def collect_fragments_pptx(path: str) -> Set[str]:
    """PPTX ‚Äî –∏–∑–≤–ª–µ–∫–∞–µ–º –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç –∏–∑ ppt/*.xml."""
    to_translate: Set[str] = set()

    with zipfile.ZipFile(path, "r") as zin:
        for info in zin.infolist():
            fname = info.filename
            low = fname.lower()

            if not low.endswith(".xml"):
                continue
            if not low.startswith("ppt/"):
                continue

            if not (
                low.startswith("ppt/slides/")
                or low.startswith("ppt/notesslides/")
                or low.startswith("ppt/slidelayouts/")
                or low.startswith("ppt/slidemasters/")
                or low == "ppt/presentation.xml"
            ):
                continue

            xml_bytes = zin.read(fname)
            frags = collect_georgian_fragments_from_xml_bytes(xml_bytes)
            to_translate.update(frags)

    print(f"üìΩÔ∏è PPTX: –Ω–∞–π–¥–µ–Ω–æ {len(to_translate)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥—Ä—É–∑–∏–Ω—Å–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.")
    return to_translate


def replace_georgian_in_xml_bytes(xml_bytes: bytes, mapping: Dict[str, str]) -> bytes:
    """–û–±—â–∞—è –∑–∞–º–µ–Ω–∞: –ø–æ–¥–º–µ–Ω—è–µ–º elem.text/elem.tail –µ—Å–ª–∏ stripped —Ä–æ–≤–Ω–æ –≤ mapping."""
    try:
        root = ET.fromstring(xml_bytes)
    except Exception:
        return xml_bytes

    def _replace(s: Optional[str]) -> Optional[str]:
        if not s:
            return s
        stripped = s.strip()
        if stripped in mapping:
            prefix_len = len(s) - len(s.lstrip())
            suffix_len = len(s) - len(s.rstrip())
            prefix = s[:prefix_len]
            suffix = s[len(s) - suffix_len:] if suffix_len > 0 else ""
            return f"{prefix}{mapping[stripped]}{suffix}"
        return s

    for elem in root.iter():
        elem.text = _replace(elem.text)
        elem.tail = _replace(elem.tail)

    return ET.tostring(root, encoding="utf-8", xml_declaration=True)


def process_docx_xml_paragraphs(xml_bytes: bytes, xml_name: str, id_mapping: Dict[str, str]) -> bytes:
    """
    –î–ª—è DOCX: –∑–∞–º–µ–Ω—è–µ–º —Ç–µ–∫—Å—Ç –∞–±–∑–∞—Ü–∞ <w:p> –ø–æ id_mapping.
    –ó–∞—â–∏—Ç–∞: –µ—Å–ª–∏ –≤ –∞–±–∑–∞—Ü–µ —É–∂–µ –Ω–µ—Ç –≥—Ä—É–∑–∏–Ω—Å–∫–∏—Ö –±—É–∫–≤ ‚Äî –Ω–µ —Ç—Ä–æ–≥–∞–µ–º.
    """
    try:
        root = ET.fromstring(xml_bytes)
    except Exception:
        return xml_bytes

    m = re.match(r"\{(.*)\}", root.tag)
    ns = m.group(1) if m else ""
    p_tag = f"{{{ns}}}p"
    t_tag = f"{{{ns}}}t"

    for p_index, p in enumerate(root.iter(p_tag)):
        para_id = f"{xml_name}::p{p_index}"
        if para_id not in id_mapping:
            continue

        t_elems = list(p.iter(t_tag))
        if not t_elems:
            continue

        orig_full = "".join([(t.text or "") for t in t_elems])
        if not orig_full:
            continue
        if not GEORGIAN_RE.search(orig_full):
            continue

        translated_clean = id_mapping[para_id]

        lead = len(orig_full) - len(orig_full.lstrip())
        trail = len(orig_full) - len(orig_full.rstrip())
        prefix = orig_full[:lead]
        suffix = orig_full[len(orig_full) - trail:] if trail > 0 else ""
        translated_full = prefix + translated_clean + suffix

        t_elems[0].text = translated_full
        for t in t_elems[1:]:
            t.text = ""

    return ET.tostring(root, encoding="utf-8", xml_declaration=True)


def debug_scan_docx_for_georgian(path: str, max_examples: int = 20) -> None:
    """–û—Ç–ª–∞–¥–∫–∞: –∏—â–µ–º –æ—Å—Ç–∞–≤—à–∏–π—Å—è –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π –≤ DOCX."""
    count = 0
    examples = []

    with zipfile.ZipFile(path, "r") as zin:
        for info in zin.infolist():
            fname = info.filename
            if not (fname.startswith("word/") and fname.lower().endswith(".xml")):
                continue

            xml_bytes = zin.read(fname)
            try:
                root = ET.fromstring(xml_bytes)
            except Exception:
                continue

            m = re.match(r"\{(.*)\}", root.tag)
            ns = m.group(1) if m else ""
            p_tag = f"{{{ns}}}p"
            t_tag = f"{{{ns}}}t"

            for p_index, p in enumerate(root.iter(p_tag)):
                full_text = "".join([(t.text or "") for t in p.iter(t_tag)])
                if not full_text:
                    continue
                if GEORGIAN_RE.search(full_text):
                    count += 1
                    if len(examples) < max_examples:
                        snippet = full_text.strip()
                        if len(snippet) > 120:
                            snippet = snippet[:117] + "..."
                        examples.append((fname, p_index, snippet))

    print(f"üîç –í —Ñ–∞–π–ª–µ {os.path.basename(path)} –æ—Å—Ç–∞–ª–æ—Å—å –∞–±–∑–∞—Ü–µ–≤ —Å –≥—Ä—É–∑–∏–Ω—Å–∫–∏–º: {count}")
    for fname, p_index, snippet in examples:
        print(f"  - {fname}::p{p_index}: {snippet}")


def apply_translations_docx(
    input_path: str,
    output_path: str,
    id_mapping: Dict[str, str],
    text_mapping: Optional[Dict[str, str]] = None,
    progress_callback: Optional[Callable[[float, str], None]] = None,
    start: float = 90.0,
    end: float = 100.0,
) -> None:
    with zipfile.ZipFile(input_path, "r") as zin, \
         zipfile.ZipFile(output_path, "w", compression=zipfile.ZIP_DEFLATED) as zout:

        infos = zin.infolist()
        total = len(infos) if infos else 1
        changed = 0

        for idx, info in enumerate(infos, start=1):
            fname = info.filename
            data = zin.read(fname)
            new_data = data

            if fname.startswith("word/") and fname.lower().endswith(".xml"):
                new_data = process_docx_xml_paragraphs(new_data, fname, id_mapping)

            if fname.lower().endswith(".xml") and text_mapping:
                new_data = replace_georgian_in_xml_bytes(new_data, text_mapping)

            if new_data != data:
                changed += 1

            zout.writestr(info, new_data)

            if progress_callback and total > 0:
                frac = idx / total
                pct = start + (end - start) * frac
                progress_callback(pct, "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ DOCX...")

    print(f"üíæ DOCX —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}, –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö XML: {changed}")


def apply_translations_xlsx(
    input_path: str,
    output_path: str,
    text_mapping: Dict[str, str],
    progress_callback: Optional[Callable[[float, str], None]] = None,
    start: float = 90.0,
    end: float = 100.0,
) -> None:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø–µ—Ä–µ–≤–æ–¥—ã –∫ XLSX —á–µ—Ä–µ–∑ openpyxl."""
    from openpyxl import load_workbook

    wb = load_workbook(input_path, data_only=False)

    total_cells = 0
    for ws in wb.worksheets:
        for row in ws.iter_rows():
            total_cells += len(row)
    if total_cells == 0:
        total_cells = 1

    processed = 0
    changed_cells = 0

    for ws in wb.worksheets:
        for row in ws.iter_rows():
            for cell in row:
                val = cell.value
                if isinstance(val, str):
                    original = val
                    stripped = val.strip()
                    if stripped in text_mapping:
                        new_core = text_mapping[stripped]

                        prefix_len = len(original) - len(original.lstrip())
                        suffix_len = len(original) - len(original.rstrip())
                        prefix = original[:prefix_len]
                        suffix = original[len(original) - suffix_len:] if suffix_len > 0 else ""

                        cell.value = f"{prefix}{new_core}{suffix}"
                        changed_cells += 1

                processed += 1
                if progress_callback:
                    frac = processed / total_cells
                    pct = start + (end - start) * frac
                    progress_callback(pct, "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ XLSX...")

    wb.save(output_path)
    print(f"üíæ XLSX —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}, –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö —è—á–µ–µ–∫: {changed_cells}")


def apply_translations_pptx(
    input_path: str,
    output_path: str,
    text_mapping: Dict[str, str],
    progress_callback: Optional[Callable[[float, str], None]] = None,
    start: float = 90.0,
    end: float = 100.0,
) -> None:
    with zipfile.ZipFile(input_path, "r") as zin, \
         zipfile.ZipFile(output_path, "w", compression=zipfile.ZIP_DEFLATED) as zout:

        infos = zin.infolist()
        total = len(infos) if infos else 1
        changed = 0

        for idx, info in enumerate(infos, start=1):
            fname = info.filename
            data = zin.read(fname)
            new_data = data

            low = fname.lower()
            if low.startswith("ppt/") and low.endswith(".xml"):
                new_data = replace_georgian_in_xml_bytes(new_data, text_mapping)

            if new_data != data:
                changed += 1

            zout.writestr(info, new_data)

            if progress_callback:
                frac = idx / total
                pct = start + (end - start) * frac
                progress_callback(pct, "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ PPTX...")

    print(f"üíæ PPTX —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}, –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö XML: {changed}")


# =============================================================================
# ChatGPT –ø–µ—Ä–µ–≤–æ–¥
# =============================================================================

def translate_with_chatgpt(
    fragments: List[str],
    model_name: str,
    api_key: str,
    target_language: str,
    progress_callback: Optional[Callable[[float, str], None]] = None,
    start: float = 10.0,
    end: float = 60.0,
) -> Dict[str, str]:
    from openai import OpenAI

    os.environ["OPENAI_API_KEY"] = api_key
    client = OpenAI()

    cleaned: List[str] = []
    for f in fragments:
        s = (f or "").strip()
        if s:
            cleaned.append(s)

    unique_texts: List[str] = []
    seen: Set[str] = set()
    for s in cleaned:
        if s not in seen:
            seen.add(s)
            unique_texts.append(s)

    if not unique_texts:
        return {}

    id_to_text: Dict[int, str] = {i: txt for i, txt in enumerate(unique_texts)}
    text_to_id: Dict[str, int] = {txt: i for i, txt in id_to_text.items()}

    print(f"–î–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —á–µ—Ä–µ–∑ ChatGPT –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(unique_texts)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.")
    fragments_struct: List[Dict[str, Any]] = [{"id": i, "text": txt} for i, txt in id_to_text.items()]

    batches = list(split_fragments_by_tokens(fragments_struct, max_tokens_per_batch=8000))
    print(f"–ë—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {len(batches)} –±–∞—Ç—á(–µ–π) –≤ –º–æ–¥–µ–ª—å {model_name}.")

    total = len(unique_texts)
    done = 0
    id_to_translated: Dict[int, str] = {}

    for batch_idx, batch in enumerate(batches, start=1):
        if progress_callback and total > 0:
            frac = done / total
            pct = start + (end - start) * frac
            progress_callback(pct, "–ü–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ ChatGPT...")

        user_payload = {
            "source_language": "Georgian",
            "target_language": target_language,
            "items": batch,
        }

        system_msg = (
            "You are a professional legal and technical translator. "
            "The texts are official documents (tariff methodology, regulatory acts, explanatory notes). "
            f"Translate from Georgian to {target_language} into natural, formal, human-quality {target_language}. "
            "You MAY freely change word order, grammar, and morphology so that the result sounds like good native legal language, "
            "but you MUST preserve all facts, numbers, names, and logical relations. "
            "Avoid literal calques from Georgian where they sound unnatural in the target language. "
            "Do NOT merge separate words together: always keep proper spaces between words, "
            "between prepositions and nouns, and around conjunctions. "
            "Fix any missing spaces if they are present in the original. "
            "Return ONLY a JSON object with a single key 'translations', whose value is a list of objects "
            "of the form {\"id\": <same id>, \"text\": <translation>}. "
            "Do not add extra fields."
        )

        max_retries = 5
        delay_seconds = 10

        for attempt in range(1, max_retries + 1):
            try:
                resp = client.chat.completions.create(
                    model=model_name,
                    response_format={"type": "json_object"},
                    messages=[
                        {"role": "system", "content": system_msg},
                        {"role": "user", "content": json.dumps(user_payload, ensure_ascii=False)},
                    ],
                )
                break
            except RateLimitError:
                print(
                    f"[Batch {batch_idx}/{len(batches)}] "
                    f"–ü–µ—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}). –ñ–¥—ë–º {delay_seconds} —Å–µ–∫—É–Ω–¥..."
                )
                if attempt == max_retries:
                    raise
                time.sleep(delay_seconds)

        content = resp.choices[0].message.content
        data = json.loads(content)

        translations_list = data.get("translations")
        if not isinstance(translations_list, list):
            raise ValueError("–û–∂–∏–¥–∞–ª—Å—è –∫–ª—é—á 'translations' —Å–æ —Å–ø–∏—Å–∫–æ–º –æ–±—ä–µ–∫—Ç–æ–≤ {id, text}.")

        for obj in translations_list:
            if not isinstance(obj, dict):
                continue
            tid = obj.get("id")
            ttext = obj.get("text")
            try:
                tid_int = int(tid)
            except (TypeError, ValueError):
                continue
            if not isinstance(ttext, str) or not ttext.strip():
                continue
            id_to_translated[tid_int] = ttext

        done += len(batch)
        print(f"   ChatGPT –ø–µ—Ä–µ–≤—ë–ª {done}/{total} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ (batch {batch_idx}/{len(batches)})")

        if progress_callback and total > 0:
            frac = done / total
            pct = start + (end - start) * frac
            progress_callback(pct, "–ü–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ ChatGPT...")

    mapping: Dict[str, str] = {}
    for txt, tid in text_to_id.items():
        trans = id_to_translated.get(tid)
        mapping[txt] = trans.strip() if isinstance(trans, str) and trans.strip() else txt

    return mapping


def post_edit_with_chatgpt(
    mapping: Dict[str, str],
    model_name: str,
    api_key: str,
    target_language: str,
    progress_callback: Optional[Callable[[float, str], None]] = None,
    start: float = 70.0,
    end: float = 90.0,
) -> Dict[str, str]:
    from openai import OpenAI

    os.environ["OPENAI_API_KEY"] = api_key
    client = OpenAI()

    unique_values: List[str] = []
    seen = set()
    for v in mapping.values():
        if isinstance(v, str) and v.strip() and v not in seen:
            seen.add(v)
            unique_values.append(v)

    if not unique_values:
        return mapping

    total = len(unique_values)
    done = 0

    batches = list(split_texts_by_tokens(unique_values, max_tokens_per_batch=8000))
    print(f"[Post-edit] –ë—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {len(batches)} –±–∞—Ç—á(–µ–π) –≤ –º–æ–¥–µ–ª—å {model_name}.")

    improved_map: Dict[str, str] = {}

    system_msg = (
        "You are a professional editor for legal, regulatory and technical documents. "
        f"Improve style, clarity, grammar and fluency in {target_language} while preserving the same meaning, facts, numbers and legal content. "
        "You MAY change word order, fix awkward literal phrases, adjust cases and morphology, "
        "replace unnatural calques with standard legal expressions, and break or merge sentences if it improves readability. "
        "Do NOT add new facts or remove existing ones. "
        "Return ONLY a JSON object mapping each original text to its improved version. "
        "Keys MUST be EXACTLY the original texts. Do not add extra fields."
    )

    max_retries = 5
    delay_seconds = 10

    for batch_idx, batch in enumerate(batches, start=1):
        if progress_callback and total > 0:
            frac = done / total
            pct = start + (end - start) * frac
            progress_callback(pct, "–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ (ChatGPT)...")

        user_payload = {"target_language": target_language, "texts": batch}

        for attempt in range(1, max_retries + 1):
            try:
                resp = client.chat.completions.create(
                    model=model_name,
                    response_format={"type": "json_object"},
                    messages=[
                        {"role": "system", "content": system_msg},
                        {"role": "user", "content": json.dumps(user_payload, ensure_ascii=False)},
                    ],
                )
                break
            except RateLimitError as e:
                print(
                    f"[Post-edit batch {batch_idx}/{len(batches)}] "
                    f"–ü–µ—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}): {e}"
                )
                if attempt == max_retries:
                    raise
                time.sleep(delay_seconds)

        data = json.loads(resp.choices[0].message.content)

        for orig_text in batch:
            new_text = data.get(orig_text)
            improved_map[orig_text] = new_text.strip() if isinstance(new_text, str) and new_text.strip() else orig_text

        done += len(batch)
        print(f"   ChatGPT –≤—ã—á–∏—Ç–∞–ª {done}/{total} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ (batch {batch_idx}/{len(batches)})")

        if progress_callback and total > 0:
            frac = done / total
            pct = start + (end - start) * frac
            progress_callback(pct, "–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ (ChatGPT)...")

    new_mapping: Dict[str, str] = {}
    for geo, raw_trans in mapping.items():
        new_mapping[geo] = improved_map.get(raw_trans, raw_trans) if isinstance(raw_trans, str) else raw_trans

    return new_mapping


# =============================================================================
# NLLB –ª–æ–∫–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: –æ—Ñ—Ñ–ª–∞–π–Ω + –∞–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
# =============================================================================

_PH_RE_CANON = re.compile(r"__PH\d+__")
_PH_RE_FUZZY = re.compile(r"__\s*PH\s*(\d+)\s*__")
_RE_KA = re.compile(r"[\u10A0-\u10FF]")
_RE_VAT_BAD_DN = re.compile(r"\b–î–ù\b", re.IGNORECASE)
_RE_VAT_KA = re.compile(r"(?:\b|\s)(·Éì·É¶·Éí)(?:-·É°|-·Éò·É°)?(?:\b)", flags=re.IGNORECASE)


def _has_ka(s: str) -> bool:
    return bool(_RE_KA.search(s or ""))


def _normalize_placeholders(s: str) -> str:
    if not isinstance(s, str) or not s:
        return s or ""
    return _PH_RE_FUZZY.sub(lambda m: f"__PH{m.group(1)}__", s)


def _placeholders_set(s: str) -> set:
    s = _normalize_placeholders(s or "")
    return set(_PH_RE_CANON.findall(s))


def _normalize_spaces(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = s.replace("\u00A0", " ")
    s = re.sub(r"[ \t]{2,}", " ", s)
    return s.strip()


def _fix_vat_terms(s: str) -> str:
    if not isinstance(s, str) or not s:
        return s or ""

    s = _RE_VAT_BAD_DN.sub("–ù–î–°", s)

    def _vat_sub(m: re.Match) -> str:
        return m.group(0).replace(m.group(1), "–ù–î–°")

    s = _RE_VAT_KA.sub(_vat_sub, s)
    s = re.sub(r"\b–ù–î–°\s*-\s*(—Å|–∏—Å)\b", "–ù–î–°", s, flags=re.IGNORECASE)
    return s


def _freeze_legal_entities(text: str) -> Tuple[str, Dict[str, str]]:
    if not isinstance(text, str) or not text:
        return text, {}

    repl: Dict[str, str] = {}
    idx = 0

    patterns = [
        r"(?:\b·Éì·É¶·Éí(?:-·É°|-–∏—Å|-·Éò·É°)?\b)",
        r"(?:(?:‚Ññ|#|N)\s?\d+(?:[/-]\d+){0,3})",
        r"(?:\b(?:Art\.|Article|—Å—Ç\.|–°—Ç–∞—Ç—å—è|–ø\.|–ø–ø\.|–ü—É–Ω–∫—Ç|–ø–∞—Ä–∞–≥—Ä–∞—Ñ)\s*\d+(?:\.\d+){0,3}\b)",
        r"(?:\b\d+(?:\.\d+){1,4}\b)",
        r"(?:\b\d{1,2}[./-]\d{1,2}[./-]\d{2,4}\b)",
        r"(?:\b\d{4}[./-]\d{1,2}[./-]\d{1,2}\b)",
        r"(?:\b\d[\d\s\u00A0]*[.,]\d+\b)",
        r"(?:\b\d+(?:[.,]\d+)?\s?(?:%|GEL|USD|EUR|kWh|MWh|GWh|kV|MW|kW|·Éö·Éê·É†·Éò|‚Çæ|\$|‚Ç¨)\b)",
        r"(?:\b\d{4,}\b)",
    ]

    big_pat = re.compile("|".join(f"(?:{p})" for p in patterns))

    def _sub(m: re.Match) -> str:
        nonlocal idx
        s = m.group(0) or ""
        s = _normalize_placeholders(s)
        if _PH_RE_CANON.fullmatch(s):
            return s
        key = f"__PH{idx}__"
        idx += 1
        repl[key] = s
        return key

    frozen = big_pat.sub(_sub, text)
    return frozen, repl


def _unfreeze_legal_entities(text: str, repl: Dict[str, str]) -> str:
    if not isinstance(text, str) or not repl:
        return text
    out = _normalize_placeholders(text)
    for k, v in repl.items():
        out = out.replace(k, v)
    return out


def _sent_split_georgian(text: str) -> List[str]:
    if not text or len(text) <= 1200:
        return [text]

    parts = re.split(r"(?<!\b\d)(?<=[\.\!\?\;\:])\s+", text)
    parts = [p.strip() for p in parts if p and p.strip()]

    chunks_out: List[str] = []
    buf = ""
    for p in parts:
        if not buf:
            buf = p
            continue
        if len(buf) + 1 + len(p) <= 1100:
            buf = buf + " " + p
        else:
            chunks_out.append(buf)
            buf = p
    if buf:
        chunks_out.append(buf)

    if len(chunks_out) > 30:
        chunks_out = []
        step = 1000
        for i in range(0, len(text), step):
            chunks_out.append(text[i:i + step])

    return chunks_out


def _rejoin_translated(chunks_in: List[str]) -> str:
    return " ".join([c.strip() for c in chunks_in if c and c.strip()]).strip()


def _load_nllb_tokenizer_and_model(
    model_name: str,
    src_lang: str,
    device: torch.device,
) -> Tuple[Any, Any]:
    """
    1) –ü—Ä–æ–±—É–µ–º —Å—Ç—Ä–æ–≥–æ OFFLINE + local_files_only=True
    2) –ï—Å–ª–∏ –≤ –∫—ç—à–µ –Ω–µ—Ç ‚Äî –≤—Ä–µ–º–µ–Ω–Ω–æ ONLINE + —Å–∫–∞—á–∏–≤–∞–µ–º
    3) –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –≤ OFFLINE
    """
    hf_enable_offline()
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=src_lang, local_files_only=True)
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name,
            local_files_only=True,
            torch_dtype=torch.float16 if device.type == "cuda" else None,
        ).to(device)
        return tokenizer, model
    except Exception:
        hf_enable_online()
        tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=src_lang, local_files_only=False)
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name,
            local_files_only=False,
            torch_dtype=torch.float16 if device.type == "cuda" else None,
        ).to(device)
        hf_enable_offline()
        return tokenizer, model


def translate_with_local_model(
    fragments: List[str],
    direction_code: str,
    progress_callback: Optional[Callable[[float, str], None]] = None,
    start: float = 10.0,
    end: float = 60.0,
) -> Dict[str, str]:
    """
    –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ª–æ–∫–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ NLLB-200-3.3B:
    - beam search (num_beams=8)
    - –∑–∞—â–∏—Ç–∞ —á–∏—Å–µ–ª/–¥–∞—Ç/—Å—É–º–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∞–º–∏
    - –±–µ–∑ –∂—ë—Å—Ç–∫–æ–≥–æ –æ—Ç–∫–∞—Ç–∞ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª
    - VAT: "·Éì·É¶·Éí/..." –∏ "–î–ù" -> "–ù–î–°"
    - –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —Å–∫–∞—á–∞–Ω–∞: 1 —Ä–∞–∑ –¥–æ–∫–∞—á–∞–µ—Ç, –ø–æ—Ç–æ–º —É–π–¥—ë—Ç –≤ –æ—Ñ—Ñ–ª–∞–π–Ω
    """
    LANG_MAP = {"ka": "kat_Geor", "ru": "rus_Cyrl", "en": "eng_Latn"}

    if "-" not in direction_code:
        raise ValueError(f"direction_code –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ñ–æ—Ä–º–∞—Ç–∞ ka-ru, –∞ –ø–æ–ª—É—á–µ–Ω–æ: {direction_code}")

    src, tgt = direction_code.split("-")
    if src not in LANG_MAP:
        raise ValueError(f"–ò—Å—Ç–æ—á–Ω–∏–∫ —è–∑—ã–∫–∞ '{src}' –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∞–Ω")
    if tgt not in LANG_MAP:
        raise ValueError(f"–¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫ '{tgt}' –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∞–Ω")

    SRC_LANG = LANG_MAP[src]
    TGT_LANG = LANG_MAP[tgt]

    remaining = [f.strip() for f in fragments if isinstance(f, str) and f.strip()]
    if not remaining:
        return {}

    MODEL_NAME = "facebook/nllb-200-3.3B"

    if progress_callback:
        progress_callback(start, f"–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ NLLB ({MODEL_NAME})‚Ä¶")
    print(f"‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é NLLB –º–æ–¥–µ–ª—å: {MODEL_NAME}")
    hf_print_mode("[Before load] ")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    tokenizer, model = _load_nllb_tokenizer_and_model(MODEL_NAME, SRC_LANG, device)
    model.eval()

    if hasattr(tokenizer, "lang_code_to_id") and TGT_LANG in tokenizer.lang_code_to_id:
        forced_bos_id = tokenizer.lang_code_to_id[TGT_LANG]
    else:
        forced_bos_id = tokenizer.convert_tokens_to_ids(TGT_LANG)

    if forced_bos_id is None or forced_bos_id < 0:
        raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å token id –¥–ª—è —è–∑—ã–∫–∞ {TGT_LANG}")

    gen_kwargs = dict(
        forced_bos_token_id=forced_bos_id,
        do_sample=False,
        num_beams=8,
        length_penalty=1.1,
        no_repeat_ngram_size=3,
        repetition_penalty=1.05,
        early_stopping=True,
        use_cache=True,
        max_new_tokens=1024,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )

    BATCH_SIZE = 4 if device.type == "cuda" else 1

    def _translate_texts(texts: List[str]) -> List[str]:
        if not texts:
            return []
        tokenizer.src_lang = SRC_LANG
        inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=False)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            generated = model.generate(**inputs, **gen_kwargs)
        outs = tokenizer.batch_decode(generated, skip_special_tokens=True)
        return [_normalize_placeholders((o or "").strip()) for o in outs]

    total = len(remaining)
    done = 0
    mapping: Dict[str, str] = {}

    for batch_idx, batch in enumerate(chunks(remaining, BATCH_SIZE), start=1):
        print(f"--> [NLLB {direction_code}] batch {batch_idx}, size={len(batch)}")

        batch_subchunks: List[List[str]] = []
        batch_freeze_meta: List[List[Dict[str, str]]] = []
        batch_raw_pieces: List[List[str]] = []
        batch_orig_texts: List[str] = []

        for orig in batch:
            batch_orig_texts.append(orig)

            pieces = _sent_split_georgian(orig)
            batch_raw_pieces.append(pieces)

            frozen_pieces: List[str] = []
            metas: List[Dict[str, str]] = []
            for p in pieces:
                fp, meta = _freeze_legal_entities(p)
                frozen_pieces.append(_normalize_placeholders(fp))
                metas.append(meta)

            batch_subchunks.append(frozen_pieces)
            batch_freeze_meta.append(metas)

        flat_texts: List[str] = [p for pieces in batch_subchunks for p in pieces]
        flat_out = _translate_texts(flat_texts)

        cursor = 0
        for orig, pieces, metas, raw_pieces in zip(batch_orig_texts, batch_subchunks, batch_freeze_meta, batch_raw_pieces):
            piece_count = len(pieces)
            out_pieces = flat_out[cursor:cursor + piece_count]
            cursor += piece_count

            unfrozen_pieces: List[str] = []
            for fp, out_text, meta, raw_p in zip(pieces, out_pieces, metas, raw_pieces):
                fp_norm = _normalize_placeholders(fp)
                t = _normalize_placeholders((out_text or "").strip())

                if not t:
                    retry = _translate_texts([fp_norm])[0] if fp_norm else ""
                    t = retry if retry else ""

                ph_before = _placeholders_set(fp_norm)
                ph_after = _placeholders_set(t)

                if ph_before and (not ph_before.issubset(ph_after)):
                    retry = _translate_texts([fp_norm])[0] if fp_norm else ""
                    if retry:
                        t = retry
                        ph_after = _placeholders_set(t)

                if ph_before and (not ph_before.issubset(ph_after)):
                    retry_raw = _translate_texts([raw_p])[0] if raw_p else ""
                    if retry_raw:
                        t = retry_raw

                t = _unfreeze_legal_entities(t, meta)
                t = _normalize_spaces(t)
                t = _fix_vat_terms(t)

                if src == "ka" and tgt != "ka" and _has_ka(t):
                    repaired = _translate_texts([t])[0]
                    if repaired and not _has_ka(repaired):
                        t = _fix_vat_terms(_normalize_spaces(repaired))

                if src == "ka" and tgt != "ka" and _has_ka(t) and raw_p:
                    repaired2 = _translate_texts([raw_p])[0]
                    if repaired2:
                        t = _fix_vat_terms(_normalize_spaces(repaired2))

                unfrozen_pieces.append(t if t else _fix_vat_terms(_normalize_spaces(_unfreeze_legal_entities(fp_norm, meta))))

            merged = _fix_vat_terms(_normalize_spaces(_rejoin_translated(unfrozen_pieces)))

            if src == "ka" and tgt != "ka" and _has_ka(merged):
                repaired = _translate_texts([merged])[0]
                if repaired and not _has_ka(repaired):
                    merged = _fix_vat_terms(_normalize_spaces(repaired))

            mapping[orig] = merged if merged else orig

        done += len(batch)
        if progress_callback and total > 0:
            pct = start + (end - start) * (done / total)
            progress_callback(pct, f"–ü–µ—Ä–µ–≤–æ–¥ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é (NLLB, {direction_code})‚Ä¶")
        print(f"   [NLLB {direction_code}] –≥–æ—Ç–æ–≤–æ {done}/{total}")

    if src == "ka" and tgt != "ka":
        left_ka = sum(1 for v in mapping.values() if _has_ka(v))
        if left_ka:
            print(f"‚ö†Ô∏è –û—Å—Ç–∞–ª–∏—Å—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å –≥—Ä—É–∑–∏–Ω—Å–∫–∏–º: {left_ka}/{len(mapping)}")
        else:
            print("‚úÖ –ì—Ä—É–∑–∏–Ω—Å–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å")

    return mapping


# =============================================================================
# –õ–æ–∫–∞–ª—å–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ Qwen (—Ç–æ–∂–µ —Å –æ—Ñ—Ñ–ª–∞–π–Ω + –∞–≤—Ç–æ—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
# =============================================================================

def _load_qwen_tokenizer_and_model(model_name: str, device: torch.device):
    hf_enable_offline()
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, local_files_only=True)
        if device.type == "cuda":
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                local_files_only=True,
            )
        else:
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                local_files_only=True,
            ).to(device)
        return tokenizer, model
    except Exception:
        hf_enable_online()
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, local_files_only=False)
        if device.type == "cuda":
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                local_files_only=False,
            )
        else:
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                local_files_only=False,
            ).to(device)
        hf_enable_offline()
        return tokenizer, model


def post_edit_with_qwen_local(
    mapping: Dict[str, str],
    target_language: str,
    progress_callback: Optional[Callable[[float, str], None]] = None,
    start: float = 60.0,
    end: float = 90.0,
) -> Dict[str, str]:
    MODEL_NAME = "Qwen/Qwen2.5-3B-Instruct"

    unique_values: List[str] = []
    seen = set()
    for v in mapping.values():
        if isinstance(v, str) and v.strip() and v not in seen:
            seen.add(v)
            unique_values.append(v)

    if not unique_values:
        return mapping

    print(f"‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –≤—ã—á–∏—Ç–∫–∏: {MODEL_NAME}")
    if progress_callback:
        progress_callback(start, "–ó–∞–≥—Ä—É–∑–∫–∞ Qwen2.5-3B-Instruct –¥–ª—è –≤—ã—á–∏—Ç–∫–∏‚Ä¶")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer, model = _load_qwen_tokenizer_and_model(MODEL_NAME, device)
    model.eval()

    system_msg_text = (
        f"You are a professional editor for {target_language} legal, regulatory and technical documents. "
        f"Improve style, clarity, grammar and fluency in {target_language} while preserving the same meaning, "
        "facts, numbers and legal content. "
        "You MAY change word order, fix awkward literal phrases, adjust cases and morphology, "
        "replace unnatural calques with standard legal expressions, and break or merge sentences if it improves readability. "
        "Do NOT add new facts or remove existing ones. "
        "Return ONLY the improved text, without explanations, without quotes."
    )

    BATCH_SIZE = 4
    total = len(unique_values)
    done = 0

    improved_map: Dict[str, str] = {}

    for batch in chunks(unique_values, BATCH_SIZE):
        for text in batch:
            messages = [
                {"role": "system", "content": system_msg_text},
                {"role": "user", "content": text},
            ]
            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(prompt, return_tensors="pt").to(device)

            with torch.no_grad():
                output_ids = model.generate(
                    **inputs,
                    max_new_tokens=512,
                    do_sample=False,
                )

            gen_ids = output_ids[0][inputs["input_ids"].shape[1]:]
            out_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()
            improved_map[text] = out_text if out_text else text

        done += len(batch)
        if progress_callback and total > 0:
            frac = done / total
            pct = start + (end - start) * frac
            progress_callback(pct, "–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ (–ª–æ–∫–∞–ª—å–Ω–∞—è Qwen)‚Ä¶")

        print(f"   Qwen –≤—ã—á–∏—Ç–∞–ª {done}/{total} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")

    new_mapping: Dict[str, str] = {}
    for geo, raw_trans in mapping.items():
        new_mapping[geo] = improved_map.get(raw_trans, raw_trans) if isinstance(raw_trans, str) else raw_trans

    return new_mapping


# =============================================================================
# –ü—Ä–æ—Å—Ç–µ–π—à–∞—è –ø—Ä–∞–≤–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
# =============================================================================

def normalize_segment_boundaries(segments: List[str]) -> List[str]:
    if len(segments) <= 1:
        return segments

    segs = segments[:]
    for i in range(len(segs) - 1):
        left = re.sub(r' {2,}', ' ', segs[i])
        right = re.sub(r' {2,}', ' ', segs[i + 1])

        if left.endswith(" ") and right.startswith(" "):
            right = right.lstrip()

        segs[i] = left
        segs[i + 1] = right

    return segs


def fix_basic_spacing_ru(text: str) -> str:
    text = text.replace("\u00A0", " ")
    text = re.sub(r'([–ê-–Ø–Å–∞-—è—ë])‚Ññ\s*(\d)', r'\1 ‚Ññ\2', text)
    text = re.sub(r'‚Ññ\s*(\d)', r'‚Ññ \1', text)
    text = re.sub(r'(?i)\b(–æ—Ç|–¥–æ|–ø–æ|–Ω–∞|–≤|–∫|—Å|—É)(\d)', r'\1 \2', text)
    text = re.sub(r'(\d)([–ê-–Ø–Å–∞-—è—ë])', r'\1 \2', text)
    text = re.sub(r'(\d{3,4})\s*(–≥–æ–¥[–∞—É–µ]?|–≥–≥?\.?)', r'\1 \2', text)
    text = re.sub(r',([^\s])', r', \1', text)
    text = re.sub(r';([^\s])', r'; \1', text)
    text = re.sub(r'([^.])\.([–ê-–Ø–Å])', r'\1. \2', text)
    text = re.sub(r'(%)([–ê-–Ø–Å–∞-—è—ë])', r'\1 \2', text)
    text = re.sub(r'[ \t]{2,}', ' ', text)
    return text


# =============================================================================
# –õ–æ–≥–∏–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
# =============================================================================

def process_file(
    file_path: str,
    translator_kind: str,
    chatgpt_model: str,
    env_path: Optional[str],
    direction_code: str,
    post_edit: bool,
    progress_callback: Optional[Callable[[float, str], None]] = None,
) -> str:
    if progress_callback is None:
        def progress_callback(pct: float, msg: str) -> None:
            pass

    if not (is_docx(file_path) or is_xlsx(file_path) or is_pptx(file_path)):
        raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã .docx, .xlsx –∏ .pptx")

    if direction_code not in DIRECTION_CONFIG:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {direction_code}")

    meta = DIRECTION_CONFIG[direction_code]
    target_language = meta["target_language"]
    suffix = meta["suffix"]

    progress_callback(0.0, "–°–±–æ—Ä –≥—Ä—É–∑–∏–Ω—Å–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")

    items_for_docx = None

    if is_docx(file_path):
        items = collect_docx_items(file_path)
        if not items:
            progress_callback(0.0, "–ì—Ä—É–∑–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            raise RuntimeError("–í —Ñ–∞–π–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞.")

        base_texts = {str(it["clean_text"]) for it in items}

        extra_texts: Set[str] = set()
        with zipfile.ZipFile(file_path, "r") as zin:
            for info in zin.infolist():
                fname = info.filename
                if not fname.lower().endswith(".xml"):
                    continue
                xml_bytes = zin.read(fname)
                extra_texts.update(collect_georgian_fragments_from_xml_bytes(xml_bytes))

        all_texts = base_texts | extra_texts
        fragments_for_translation = sorted(t for t in all_texts if t.strip() and GEORGIAN_RE.search(t))
        items_for_docx = items

    elif is_pptx(file_path):
        fragments_set = collect_fragments_pptx(file_path)
        if not fragments_set:
            progress_callback(0.0, "–ì—Ä—É–∑–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            raise RuntimeError("–í —Ñ–∞–π–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞.")
        fragments_for_translation = sorted(fragments_set)

    else:
        fragments_set = collect_fragments_xlsx(file_path)
        if not fragments_set:
            progress_callback(0.0, "–ì—Ä—É–∑–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            raise RuntimeError("–í —Ñ–∞–π–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω –≥—Ä—É–∑–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞.")
        fragments_for_translation = sorted(fragments_set)

    print(f"–ù–∞–π–¥–µ–Ω–æ {len(fragments_for_translation)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞.")
    progress_callback(5.0, f"–ù–∞–π–¥–µ–Ω–æ {len(fragments_for_translation)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –ø–µ—Ä–µ–≤–æ–¥—É...")

    if translator_kind == "chatgpt":
        if not env_path:
            raise ValueError("–ù–µ –≤—ã–±—Ä–∞–Ω .env —Ñ–∞–π–ª —Å —Ç–æ–∫–µ–Ω–æ–º –¥–ª—è ChatGPT.")
        api_key = load_api_key_from_env_file(env_path)

        mapping_text_to_trans = translate_with_chatgpt(
            fragments_for_translation,
            chatgpt_model,
            api_key,
            target_language,
            progress_callback=progress_callback,
            start=10.0,
            end=60.0,
        )

        if post_edit:
            mapping_text_to_trans = post_edit_with_chatgpt(
                mapping_text_to_trans,
                chatgpt_model,
                api_key,
                target_language,
                progress_callback=progress_callback,
                start=60.0,
                end=90.0,
            )
    else:
        mapping_text_to_trans = translate_with_local_model(
            fragments_for_translation,
            direction_code,
            progress_callback=progress_callback,
            start=10.0,
            end=60.0,
        )

        if post_edit:
            mapping_text_to_trans = post_edit_with_qwen_local(
                mapping_text_to_trans,
                target_language=target_language,
                progress_callback=progress_callback,
                start=60.0,
                end=90.0,
            )

    # DOCX: –Ω—É–∂–Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ id –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ -> –ø–µ—Ä–µ–≤–æ–¥
    if is_docx(file_path):
        id_mapping: Dict[str, str] = {}
        for it in items_for_docx:  # type: ignore
            clean_text = str(it["clean_text"])
            item_id = str(it["id"])
            translated = mapping_text_to_trans.get(clean_text, clean_text)
            id_mapping[item_id] = translated
    else:
        id_mapping = {}  # –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è xlsx/pptx

    base, ext = os.path.splitext(file_path)
    output_path = f"{base}{suffix}{ext}"

    progress_callback(90.0, "–ü—Ä–∏–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã –∫ —Ñ–∞–π–ª—É...")

    if is_docx(file_path):
        apply_translations_docx(
            file_path,
            output_path,
            id_mapping,
            mapping_text_to_trans,
            progress_callback=progress_callback,
            start=90.0,
            end=100.0,
        )
    elif is_pptx(file_path):
        apply_translations_pptx(
            file_path,
            output_path,
            mapping_text_to_trans,
            progress_callback=progress_callback,
            start=90.0,
            end=100.0,
        )
    else:
        # –í–ê–ñ–ù–û: —Ç—É—Ç –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∏–º–µ–Ω–Ω–æ mapping_text_to_trans, –∞ –Ω–µ id_mapping
        apply_translations_xlsx(
            file_path,
            output_path,
            mapping_text_to_trans,
            progress_callback=progress_callback,
            start=90.0,
            end=100.0,
        )

    progress_callback(100.0, "–ì–æ—Ç–æ–≤–æ.")

    if is_docx(output_path):
        debug_scan_docx_for_georgian(output_path)

    return output_path


# =============================================================================
# GUI (Tkinter)
# =============================================================================

class TranslatorGUI:
    def __init__(self, root: tk.Tk):
        self.root = root
        self.root.title("–ü–µ—Ä–µ–≤–æ–¥ –≥—Ä—É–∑–∏–Ω—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ DOCX/XLSX/PPTX")

        self.file_path_var = tk.StringVar()
        self.env_path_var = tk.StringVar()
        self.translator_var = tk.StringVar(value="chatgpt")
        self.model_var = tk.StringVar(value=DEFAULT_CHATGPT_MODEL)
        self.direction_label_var = tk.StringVar(value=DIRECTION_CONFIG["ka-ru"]["label"])

        self.progress_var = tk.DoubleVar(value=0.0)
        self.status_var = tk.StringVar(value="–ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ.")

        self.post_edit_var = tk.BooleanVar(value=False)

        self.start_button: Optional[ttk.Button] = None
        self.post_edit_check: Optional[ttk.Checkbutton] = None

        self.direction_combo: Optional[ttk.Combobox] = None
        self.model_combo: Optional[ttk.Combobox] = None
        self.env_entry: Optional[ttk.Entry] = None
        self.env_button: Optional[ttk.Button] = None
        self.progress_bar: Optional[ttk.Progressbar] = None
        self.status_label: Optional[ttk.Label] = None

        self.build_ui()

    def build_ui(self):
        pad = 5

        frm = ttk.Frame(self.root, padding=10)
        frm.grid(row=0, column=0, sticky="nsew")

        ttk.Label(frm, text="–§–∞–π–ª DOCX/XLSX/PPTX:").grid(row=0, column=0, sticky="w", pady=pad)
        entry_file = ttk.Entry(frm, textvariable=self.file_path_var, width=60)
        entry_file.grid(row=0, column=1, sticky="we", pady=pad)
        ttk.Button(frm, text="–í—ã–±—Ä–∞—Ç—å...", command=self.choose_file).grid(row=0, column=2, padx=pad, pady=pad)

        ttk.Label(frm, text="–ü–µ—Ä–µ–≤–æ–¥—á–∏–∫:").grid(row=1, column=0, sticky="w", pady=pad)

        r1 = ttk.Radiobutton(
            frm,
            text="ChatGPT (–æ–±–ª–∞—á–Ω—ã–π)",
            variable=self.translator_var,
            value="chatgpt",
            command=self.on_translator_change,
        )
        r1.grid(row=1, column=1, sticky="w", pady=pad)

        r2 = ttk.Radiobutton(
            frm,
            text="–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (NLLB-200)",
            variable=self.translator_var,
            value="local",
            command=self.on_translator_change,
        )
        r2.grid(row=2, column=1, sticky="w", pady=pad)

        ttk.Label(frm, text="–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞:").grid(row=3, column=0, sticky="w", pady=pad)
        direction_values = [meta["label"] for meta in DIRECTION_CONFIG.values()]
        self.direction_combo = ttk.Combobox(
            frm,
            textvariable=self.direction_label_var,
            values=direction_values,
            state="readonly",
            width=35,
        )
        self.direction_combo.grid(row=3, column=1, sticky="w", pady=pad)

        ttk.Label(frm, text="–ú–æ–¥–µ–ª—å ChatGPT:").grid(row=4, column=0, sticky="w", pady=pad)
        self.model_combo = ttk.Combobox(
            frm,
            textvariable=self.model_var,
            values=CHATGPT_MODELS,
            state="readonly",
            width=35,
        )
        self.model_combo.grid(row=4, column=1, sticky="w", pady=pad)

        ttk.Label(frm, text=".env —Å —Ç–æ–∫–µ–Ω–æ–º:").grid(row=5, column=0, sticky="w", pady=pad)
        self.env_entry = ttk.Entry(frm, textvariable=self.env_path_var, width=60)
        self.env_entry.grid(row=5, column=1, sticky="we", pady=pad)
        self.env_button = ttk.Button(frm, text="–í—ã–±—Ä–∞—Ç—å .env...", command=self.choose_env_file)
        self.env_button.grid(row=5, column=2, padx=pad, pady=pad)

        self.post_edit_check = ttk.Checkbutton(
            frm,
            text="–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ (—É–ª—É—á—à–∞—Ç—å —Å—Ç–∏–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞)",
            variable=self.post_edit_var,
        )
        self.post_edit_check.grid(row=6, column=0, columnspan=3, sticky="w", pady=pad)

        ttk.Label(frm, text="–ü—Ä–æ–≥—Ä–µ—Å—Å:").grid(row=7, column=0, sticky="w", pady=pad)
        self.progress_bar = ttk.Progressbar(
            frm,
            maximum=100.0,
            variable=self.progress_var,
            mode="determinate",
            length=300,
        )
        self.progress_bar.grid(row=7, column=1, columnspan=2, sticky="we", pady=pad)

        self.status_label = ttk.Label(frm, textvariable=self.status_var)
        self.status_label.grid(row=8, column=0, columnspan=3, sticky="w", pady=pad)

        self.start_button = ttk.Button(frm, text="–°—Ç–∞—Ä—Ç –ø–µ—Ä–µ–≤–æ–¥–∞", command=self.run_translation)
        self.start_button.grid(row=9, column=0, columnspan=3, pady=10)

        self.root.columnconfigure(0, weight=1)
        frm.columnconfigure(1, weight=1)

        self.on_translator_change()

    def _update_progress_mainthread(self, pct: float, msg: str) -> None:
        self.progress_var.set(max(0.0, min(100.0, pct)))
        self.status_var.set(msg)

    def set_progress(self, pct: float, msg: str) -> None:
        self.root.after(0, self._update_progress_mainthread, pct, msg)

    def choose_file(self):
        path = filedialog.askopenfilename(
            title="–í—ã–±–µ—Ä–∏—Ç–µ DOCX/XLSX/PPTX",
            filetypes=[("Office files", "*.docx *.xlsx *.pptx"), ("–í—Å–µ —Ñ–∞–π–ª—ã", "*.*")],
        )
        if path:
            self.file_path_var.set(path)

    def choose_env_file(self):
        path = filedialog.askopenfilename(
            title="–í—ã–±–µ—Ä–∏—Ç–µ .env —Ñ–∞–π–ª —Å OPENAI_API_KEY",
            filetypes=[("ENV files", "*.env;*.txt;*.*"), ("–í—Å–µ —Ñ–∞–π–ª—ã", "*.*")],
        )
        if path:
            self.env_path_var.set(path)

    def on_translator_change(self):
        kind = self.translator_var.get()
        if kind == "chatgpt":
            if self.model_combo is not None:
                self.model_combo.configure(state="readonly")
            if self.env_entry is not None:
                self.env_entry.configure(state="normal")
            if self.env_button is not None:
                self.env_button.configure(state="normal")
            if self.post_edit_check is not None:
                self.post_edit_check.configure(text="–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ (—á–µ—Ä–µ–∑ ChatGPT)", state="normal")
        else:
            if self.model_combo is not None:
                self.model_combo.configure(state="disabled")
            if self.env_entry is not None:
                self.env_entry.configure(state="disabled")
            if self.env_button is not None:
                self.env_button.configure(state="disabled")
            if self.post_edit_check is not None:
                self.post_edit_check.configure(text="–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –≤—ã—á–∏—Ç–∫–∞ (–ª–æ–∫–∞–ª—å–Ω–∞—è Qwen2.5-3B-Instruct)", state="normal")

    def run_translation(self):
        file_path = self.file_path_var.get().strip()
        if not file_path:
            messagebox.showerror("–û—à–∏–±–∫–∞", "–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª DOCX/XLSX/PPTX.")
            return

        if not (is_docx(file_path) or is_xlsx(file_path) or is_pptx(file_path)):
            messagebox.showerror("–û—à–∏–±–∫–∞", "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã .docx, .xlsx –∏ .pptx.")
            return

        translator_kind = self.translator_var.get()
        chatgpt_model = self.model_var.get()
        env_path = self.env_path_var.get().strip() if translator_kind == "chatgpt" else None
        post_edit = bool(self.post_edit_var.get())

        direction_label = self.direction_label_var.get()
        try:
            direction_code = get_direction_code_from_label(direction_label)
        except ValueError as e:
            messagebox.showerror("–û—à–∏–±–∫–∞", str(e))
            return

        if translator_kind == "chatgpt" and not env_path:
            messagebox.showerror("–û—à–∏–±–∫–∞", "–í—ã–±–µ—Ä–∏—Ç–µ .env —Ñ–∞–π–ª —Å —Ç–æ–∫–µ–Ω–æ–º –¥–ª—è ChatGPT.")
            return

        if self.start_button is not None:
            self.start_button.configure(state="disabled")
        self.set_progress(0.0, "–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏...")

        t = threading.Thread(
            target=self._worker_translate,
            args=(file_path, translator_kind, chatgpt_model, env_path, direction_code, post_edit),
            daemon=True,
        )
        t.start()

    def _worker_translate(
        self,
        file_path: str,
        translator_kind: str,
        chatgpt_model: str,
        env_path: Optional[str],
        direction_code: str,
        post_edit: bool,
    ):
        try:
            output_path = process_file(
                file_path=file_path,
                translator_kind=translator_kind,
                chatgpt_model=chatgpt_model,
                env_path=env_path,
                direction_code=direction_code,
                post_edit=post_edit,
                progress_callback=self.set_progress,
            )
        except Exception as e:
            import traceback
            traceback.print_exc()

            err_msg = f"{type(e).__name__}: {e}"

            def show_error(msg=err_msg):
                if self.start_button is not None:
                    self.start_button.configure(state="normal")
                self._update_progress_mainthread(0.0, "–û—à–∏–±–∫–∞.")
                messagebox.showerror("–û—à–∏–±–∫–∞", f"–ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π:\n{msg}")

            self.root.after(0, show_error)
            return

        def on_done():
            if self.start_button is not None:
                self.start_button.configure(state="normal")
            self._update_progress_mainthread(100.0, "–ì–æ—Ç–æ–≤–æ.")
            messagebox.showinfo("–ì–æ—Ç–æ–≤–æ", f"–§–∞–π–ª –ø–µ—Ä–µ–≤–µ–¥—ë–Ω:\n{output_path}")

        self.root.after(0, on_done)


def main():
    root = tk.Tk()
    TranslatorGUI(root)
    root.mainloop()


if __name__ == "__main__":
    main()
